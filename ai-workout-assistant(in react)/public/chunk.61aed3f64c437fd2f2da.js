/*! For license information please see chunk.61aed3f64c437fd2f2da.js.LICENSE.txt */
"use strict";(self.webpackChunkai_workout_assistant=self.webpackChunkai_workout_assistant||[]).push([[73],{40588:(t,e,s)=>{s.d(e,{LH:()=>n,j1:()=>i,nj:()=>a,nu:()=>r,ps:()=>l});class i extends Error{constructor(t){super(t),Object.setPrototypeOf(this,i.prototype)}}class n extends Error{constructor(t){super(t),Object.setPrototypeOf(this,n.prototype)}}class r extends Error{constructor(t){super(t),Object.setPrototypeOf(this,r.prototype)}}class a extends Error{constructor(t){super(t),Object.setPrototypeOf(this,a.prototype)}}class l extends Error{constructor(t){super(t),Object.setPrototypeOf(this,l.prototype)}}class o extends Error{constructor(t){super(t),Object.setPrototypeOf(this,o.prototype)}}},93202:(t,e,s)=>{s.d(e,{FB:()=>f});var i=s(88478),n=s(64843);(0,i.OBj)().registerFlag("TOPOLOGICAL_SORT_CACHE_MAX_ENTRIES",(()=>100),n.kS),s(88940);var r=s(64079),a=s(85654),l=s(34396),o=s(20163),u=(s(28891),s(5540)),h=s(79608),c=s(40588),p=s(49897),d=s(2931),g=s(51977),m=s(87538);async function f(t,e){if(null==e&&(e={}),"string"==typeof t){const s=i.io.getLoadHandlers(t,e);if(0===s.length)s.push(i.io.browserHTTPRequest(t,e));else if(s.length>1)throw new c.nu(`Found more than one (${s.length}) load handlers for URL '${t}'`);t=s[0]}return async function(t,e,s){if(null==s&&(s={}),null==t.load)throw new c.nu("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const n=await t.load();let r=n.modelTopology;null!=r.model_config&&(r=r.model_config);const a=null==s.strict||s.strict,l=null!=n.weightData&&null!=n.weightSpecs&&a,o=(0,p.v)((0,g.a)(r),void 0,l),u=n.trainingConfig;if(null!=u&&o.loadTrainingConfig(u),null!=n.userDefinedMetadata&&o.setUserDefinedMetadata(n.userDefinedMetadata),null!=n.weightData){if(null==n.weightSpecs)throw new c.nu("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:e}=function(t,e){const s=i.io.decodeWeights(t,e),n={},r=[];return e.forEach((t=>{"optimizer"===t.group?r.push({name:t.name,tensor:s[t.name]}):n[t.name]=s[t.name]})),{modelWeights:n,optimizerWeights:r}}(n.weightData,n.weightSpecs);o.loadWeights(t,a),null!=o.optimizer&&e.length>0&&await o.optimizer.setWeights(e),(0,i.B90)(t),(0,i.B90)(e.map((t=>t.tensor)))}return o}(t,0,e)}class b extends u.QV{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:(0,h.s)("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some((t=>t<0)))throw new c.nu(`Negative dimension size caused by adding layer ${t.name} with input shape [${t.inboundNodes[0].inputTensors[0].shape}]`)}add(t){const e=t instanceof b||t instanceof u.QV;let s;if(e){if(s=t,1!==s.outputs.length)throw new c.nu("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==s.inputs.length)throw new c.nu("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new c.nu("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=(0,l.I)({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=s.outputs,this.inputs=s.inputs;else{if(1!==t.inboundNodes.length)throw new c.nu(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${t.name} which has ${t.inboundNodes.length} pre-existing inbound connections.`);if(1!==t.inboundNodes[0].outputTensors.length)throw new c.nu("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=(0,o.hA)(this.outputs[0])}this.inboundNodes=[],new o.NB({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:d.JE(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if((0,m.Wf)(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new u.QV({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e,s=console.log){this.built||this.build(),super.summary(t,e,s)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e,s={}){if(!this.built)throw new c.LH("The model needs to be compiled before being used.");return this.model.evaluate(t,e,s)}async evaluateDataset(t,e){if(!this.built)throw new c.LH("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t,e={}){return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e,s={}){if(!this.built)throw new c.LH("The model needs to be compiled before being used.");return this.model.fit(t,e,s)}async fitDataset(t,e){if(!this.built)throw new c.LH("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,e,s={},n=!1){let r,a={};if(e instanceof Array){if(null==e[0].className||"Merge"===e[0].className)throw new c.nu("Legacy serialization format not supported yet.");r=e}else i.D5U.assert(null!=e.layers,(()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.")),r=e.layers,delete e.layers,a=e;const l=new t(a);if(!(l instanceof b))throw new c.nj(`Sequential.fromConfig called on non-Sequential input: ${l}`);for(const t of r){const e=void 0,s=(0,p.v)(t,e,n);n&&s.setFastWeightInitDuringBuild(!0),l.add(s)}return l}set stopTraining(t){if(null==this.model)throw new c.nu("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new c.nu("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const s={};s.className=e.getClassName(),s.config=e.getConfig(),t.push(s)}return{name:this.name,layers:t}}}b.className="Sequential",i.m7h.registerClass(b);var y=s(28819),w=s(39840);class C extends i.m7h.Serializable{}class S extends C{constructor(t){super(),function(t){if(null!=t&&"object"!=typeof t)throw new Error(`Argument to L1L2 regularizer's constructor is expected to be an object, but received: ${t}`)}(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return(0,i.lub)((()=>{let e=(0,i.lls)([1]);return this.hasL1&&(e=(0,i.IHx)(e,(0,i.Smz)(i.dC7(this.l1,(0,i.WnP)(t))))),this.hasL2&&(e=(0,i.IHx)(e,(0,i.Smz)(i.dC7(this.l2,w.h6(t))))),i.XLQ(e,[])}))}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}S.className="L1L2",i.m7h.registerClass(S);const z={l1l2:"L1L2"};function k(t){return(0,d.Kj)(t)}function A(t,e={}){return(0,d.tU)(t,i.m7h.SerializationMap.getMap().classNameMap,e,"regularizer")}function I(t){return null==t?null:"string"==typeof t?A({className:t in z?z[t]:t,config:{}}):t instanceof C?t:A(t)}class v extends o.mh{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,e){t=(0,m.nQ)(t);let s=(0,i.UYe)(t);return null!=this.maxValue&&(s=(0,i.iUl)(s,0,this.maxValue)),s}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}v.className="ReLU",i.m7h.registerClass(v);class N extends o.mh{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const s=(0,m.nQ)(t);return(0,i.hi7)(s,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}N.className="LeakyReLU",i.m7h.registerClass(N);class x extends o.mh{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=(0,a.L5)(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=I(t.alphaRegularizer),this.alphaConstraint=(0,r.Ad)(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new c.nu(`Expected sharedAxes to be a number or an array of numbers, but got ${t.sharedAxes}`);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=(0,m.Wf)(t)).slice(1);if(null!=this.sharedAxes)for(const t of this.sharedAxes)e[t-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const s={};if(null!=this.sharedAxes)for(let e=1;e<t.length;++e)s[e]=t[e];this.inputSpec=[new o.Zg({ndim:t.length,axes:s})],this.built=!0}call(t,e){return t=(0,m.nQ)(t),(0,i.AL3)(t,this.alpha.read())}getConfig(){const t={alphaInitializer:(0,a.Cx)(this.alphaInitializer),alphaRegularizer:k(this.alphaRegularizer),alphaConstraint:(0,r.xF)(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}x.className="PReLU",i.m7h.registerClass(x);class F extends o.mh{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new c.nj(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const s=(0,m.nQ)(t);return(0,i.pyx)(s)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}F.className="ELU",i.m7h.registerClass(F);class L extends o.mh{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const s=(0,m.nQ)(t);return(0,i.dC7)(s,(0,i.pju)((0,i.pjt)(s,this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}L.className="ThresholdedReLU",i.m7h.registerClass(L);class D extends o.mh{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new y.Gc).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){const s=(0,m.nQ)(t);return this.softmax(s,this.axis)}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}D.className="Softmax",i.m7h.registerClass(D);var R=s(12012),T=s(48090),E=s(96040);function O(t,e,s){if("number"==typeof t)return(0,d.JE)(t,e);if(t.length!==e)throw new c.nu(`The ${s} argument must be an integer or tuple of ${e} integers. Received: ${t.length} elements.`);for(let i=0;i<e;++i){const n=t[i];if(!(0,E.U)(n))throw new c.nu(`The ${s} argument must be an integer or tuple of ${e} integers. Received: ${JSON.stringify(t)} including a non-integer number ${n}`)}return t}function M(t,e,s,i,n=1){if(null==t)return t;let r;return r="same"===s?t:t-(e+(e-1)*(n-1))+1,Math.floor((r+i-1)/i)}function U(t,e,s,i){if(null==t)return null;if("valid"===i)t=t*e+(0,E.Fp)([s-e,0]);else{if("same"!==i)throw new c.nu(`Unsupport padding mode: ${i}.`);t*=e}return t}function j(t,e){return(0,i.lub)((()=>((0,T.cj)(e),"channelsFirst"===e?i.p4s(t,[0,2,3,1]):t)))}function _(t,e){return(0,i.lub)((()=>((0,T.cj)(e),"channelsFirst"===e?i.p4s(t,[0,2,3,4,1]):t)))}function $(t,e,s,n=[1,1],r="valid",a,l,o=null){return(0,i.lub)((()=>{if(null==a&&(a=(0,R.rf)()),(0,T.cj)(a),3!==t.rank&&4!==t.rank)throw new c.nu(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${t.rank}.`);if(3!==e.rank&&4!==e.rank)throw new c.nu(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${t.rank}.`);let u=j(t,a);if("causal"===r)throw new c.nj("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return u=i.imm.conv2d({x:u,filter:e,strides:n,pad:"same"===r?"same":"valid",dilations:l,dataFormat:"NHWC",bias:s,activation:o}),"channelsFirst"===a&&(u=i.p4s(u,[0,3,1,2])),u}))}class W extends o.mh{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",W.verifyArgs(e),this.rank=t,d.iQ(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new c.nj(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=O(e.kernelSize,t,"kernelSize"),this.strides=O(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,(0,T.zb)(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,(0,T.cj)(this.dataFormat),this.activation=(0,y.aI)(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=(0,a.L5)(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=(0,r.Ad)(e.biasConstraint),this.biasRegularizer=I(e.biasRegularizer),this.activityRegularizer=I(e.activityRegularizer),this.dilationRate=O(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new c.nu(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new c.nu(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new c.nu(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(t){if(d.hu("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!=typeof t.kernelSize&&!d.Mx(t.kernelSize,"number",1,3))throw new c.nu(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(t.kernelSize)}.`)}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:(0,y.GD)(this.activation),useBias:this.useBias,biasInitializer:(0,a.Cx)(this.biasInitializer),biasRegularizer:k(this.biasRegularizer),activityRegularizer:k(this.activityRegularizer),biasConstraint:(0,r.xF)(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class B extends W{constructor(t,e){super(t,e),this.kernel=null,B.verifyArgs(e),this.filters=e.filters,d.iQ(this.filters,"filters"),this.kernelInitializer=(0,a.L5)(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=(0,r.Ad)(e.kernelConstraint),this.kernelRegularizer=I(e.kernelRegularizer)}build(t){t=(0,m.Wf)(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new c.nu(`The channel dimension of the input should be defined. Found ${t[e]}`);const s=t[e],i=this.kernelSize.concat([s,this.filters]);this.kernel=this.addWeight("kernel",i,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:s}}],this.built=!0}call(t,e){return(0,i.lub)((()=>{let e;t=(0,m.nQ)(t);const s=null==this.bias?null:this.bias.read(),n=d.WT(this.activation.getClassName());if(null!=n&&2===this.rank)e=$(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate,n);else{if(1===this.rank)e=function(t,e,s,n=1,r="valid",a,l=1){return(0,i.lub)((()=>{if(null==a&&(a=(0,R.rf)()),(0,T.cj)(a),3!==t.shape.length)throw new c.nu(`The input of a conv1dWithBias operation should be 3, but is ${t.shape.length} instead.`);if(3!==e.shape.length)throw new c.nu(`The kernel for a conv1dWithBias operation should be 3, but is ${e.shape.length} instead`);if(null!=s&&1!==s.shape.length)throw new c.nu(`The bias for a conv1dWithBias operation should be 1, but is ${e.shape.length} instead`);if("channelsFirst"===a&&(t=i.p4s(t,[0,2,1])),"causal"===r)throw new c.nj("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let o=i.PAt(t,e,n,"same"===r?"same":"valid","NWC",l);return null!=s&&(o=w.a2(o,s)),o}))}(t,this.kernel.read(),s,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)e=$(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new c.nj("convolutions greater than 3D are not implemented yet.");e=function(t,e,s,n=[1,1,1],r="valid",a,l){return(0,i.lub)((()=>{if(null==a&&(a=(0,R.rf)()),(0,T.cj)(a),4!==t.rank&&5!==t.rank)throw new c.nu(`conv3dWithBias expects input to be of rank 4 or 5, but received ${t.rank}.`);if(4!==e.rank&&5!==e.rank)throw new c.nu(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${t.rank}.`);let o=_(t,a);if("causal"===r)throw new c.nj("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return o=i.pdZ(o,e,n,"same"===r?"same":"valid","NDHWC",l),null!=s&&(o=w.a2(o,s)),"channelsFirst"===a&&(o=i.p4s(o,[0,4,1,2,3])),o}))}(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(e=this.activation.apply(e))}return e}))}computeOutputShape(t){t=(0,m.Wf)(t);const e=[],s="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let t=0;t<s.length;++t){const i=M(s[t],this.kernelSize[t],this.padding,this.strides[t],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[t]);e.push(i)}let i=[t[0]];return"channelsLast"===this.dataFormat?(i=i.concat(e),i.push(this.filters)):(i.push(this.filters),i=i.concat(e)),i}getConfig(){const t={filters:this.filters,kernelInitializer:(0,a.Cx)(this.kernelInitializer),kernelRegularizer:k(this.kernelRegularizer),kernelConstraint:(0,r.xF)(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!=typeof t.filters||t.filters<1)throw new c.nu(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(t.filters)}`)}}class H extends B{constructor(t){super(2,t),H.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!d.Mx(t.kernelSize,"number",1,2))throw new c.nu(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}H.className="Conv2D",i.m7h.registerClass(H);class Q extends B{constructor(t){super(3,t),Q.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new c.nu(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}Q.className="Conv3D",i.m7h.registerClass(Q);class V extends H{constructor(t){if(super(t),this.inputSpec=[new o.Zg({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new c.nu(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(4!==(t=(0,m.Wf)(t)).length)throw new c.nu("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new c.nu("The channel dimension of the inputs should be defined. Found `None`.");const s=t[e],i=this.kernelSize.concat([this.filters,s]);this.kernel=this.addWeight("kernel",i,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new o.Zg({ndim:4,axes:{[e]:s}})],this.built=!0}call(t,e){return i.lub((()=>{let e=(0,m.nQ)(t);if(4!==e.shape.length)throw new c.nu(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${e.shape.length}`);const s=e.shape,n=s[0];let r,a;"channelsFirst"===this.dataFormat?(r=2,a=3):(r=1,a=2);const l=s[r],o=s[a],u=this.kernelSize[0],h=this.kernelSize[1],p=this.strides[0],d=this.strides[1],g=[n,U(l,p,u,this.padding),U(o,d,h,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=i.p4s(e,[0,2,3,1]));let f=i.bc(e,this.kernel.read(),g,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(f=i.p4s(f,[0,3,1,2])),null!=this.bias&&(f=w.a2(f,this.bias.read(),this.dataFormat)),null!=this.activation&&(f=this.activation.apply(f)),f}))}computeOutputShape(t){const e=(t=(0,m.Wf)(t)).slice();let s,i,n;"channelsFirst"===this.dataFormat?(s=1,i=2,n=3):(s=3,i=1,n=2);const r=this.kernelSize[0],a=this.kernelSize[1],l=this.strides[0],o=this.strides[1];return e[s]=this.filters,e[i]=U(e[i],l,r,this.padding),e[n]=U(e[n],o,a,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}V.className="Conv2DTranspose",i.m7h.registerClass(V);class J extends Q{constructor(t){if(super(t),this.inputSpec=[new o.Zg({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new c.nu(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(5!==(t=(0,m.Wf)(t)).length)throw new c.nu("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new c.nu("The channel dimension of the inputs should be defined. Found `None`.");const s=t[e],i=this.kernelSize.concat([this.filters,s]);this.kernel=this.addWeight("kernel",i,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new o.Zg({ndim:5,axes:{[e]:s}})],this.built=!0}call(t,e){return i.lub((()=>{let e=(0,m.nQ)(t);if(5!==e.shape.length)throw new c.nu(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${e.shape.length}`);const s=e.shape,n=s[0];let r,a,l;"channelsFirst"===this.dataFormat?(l=2,r=3,a=4):(l=1,r=2,a=3);const o=s[l],u=s[r],h=s[a],p=this.kernelSize[0],d=this.kernelSize[1],g=this.kernelSize[2],f=this.strides[0],b=this.strides[1],y=this.strides[2],C=[n,U(o,f,p,this.padding),U(u,b,d,this.padding),U(h,y,g,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=i.p4s(e,[0,2,3,4,1]));let S=i.$QV(e,this.kernel.read(),C,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(S=i.p4s(S,[0,4,1,2,3])),null!==this.bias&&(S=w.a2(S,this.bias.read(),this.dataFormat)),null!==this.activation&&(S=this.activation.apply(S)),S}))}computeOutputShape(t){const e=(t=(0,m.Wf)(t)).slice();let s,i,n,r;"channelsFirst"===this.dataFormat?(s=1,i=2,n=3,r=4):(s=4,i=1,n=2,r=3);const a=this.kernelSize[0],l=this.kernelSize[1],o=this.kernelSize[2],u=this.strides[0],h=this.strides[1],c=this.strides[2];return e[s]=this.filters,e[i]=U(e[i],u,a,this.padding),e[n]=U(e[n],h,l,this.padding),e[r]=U(e[r],c,o,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}J.className="Conv3DTranspose",i.m7h.registerClass(J);class Z extends B{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new c.nu("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new c.nu("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new c.nu(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(e.padding)}`);this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=(0,a.L5)(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=I(e.depthwiseRegularizer),this.depthwiseConstraint=(0,r.Ad)(e.depthwiseConstraint),this.pointwiseInitializer=(0,a.L5)(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=I(e.pointwiseRegularizer),this.pointwiseConstraint=(0,r.Ad)(e.pointwiseConstraint)}build(t){if((t=(0,m.Wf)(t)).length<this.rank+2)throw new c.nu(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(t)}`);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new c.nu(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(t[e])}`);const s=t[e],i=this.kernelSize.concat([s,this.depthMultiplier]),n=[];for(let t=0;t<this.rank;++t)n.push(1);n.push(s*this.depthMultiplier,this.filters);const r=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",i,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,r,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",n,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,r,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,r,this.biasConstraint):this.bias=null,this.inputSpec=[new o.Zg({ndim:this.rank+2,axes:{[e]:s}})],this.built=!0}call(t,e){return(0,i.lub)((()=>{let e;if(t=(0,m.nQ)(t),1===this.rank)throw new c.nj("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=i.p4s(t,[0,2,3,1])),e=i.U_I(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(e=w.a2(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),"channelsFirst"===this.dataFormat&&(e=i.p4s(e,[0,3,1,2])),e}))}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=(0,a.Cx)(this.depthwiseInitializer),t.pointwiseInitializer=(0,a.Cx)(this.pointwiseInitializer),t.depthwiseRegularizer=k(this.depthwiseRegularizer),t.pointwiseRegularizer=k(this.pointwiseRegularizer),t.depthwiseConstraint=(0,r.xF)(this.depthwiseConstraint),t.pointwiseConstraint=(0,r.xF)(this.pointwiseConstraint),t}}Z.className="SeparableConv";class P extends Z{constructor(t){super(2,t)}}P.className="SeparableConv2D",i.m7h.registerClass(P);class q extends B{constructor(t){super(1,t),q.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!d.Mx(t.kernelSize,"number",1,1))throw new c.nu(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}q.className="Conv1D",i.m7h.registerClass(q);class G extends o.mh{constructor(t){super(t),"number"==typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,e){return(0,i.lub)((()=>{if(t=(0,m.nQ)(t),"channelsLast"===this.dataFormat){const e=w.uI(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return w.uI(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=w.uI(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return w.uI(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}}))}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}G.className="Cropping2D",i.m7h.registerClass(G);class K extends o.mh{constructor(t){super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,T.cj)(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,(0,T.wU)(this.interpolation)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],s=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,s]}{const e=null==t[1]?null:this.size[0]*t[1],s=null==t[2]?null:this.size[1]*t[2];return[t[0],e,s,t[3]]}}call(t,e){return i.lub((()=>{let e=(0,m.nQ)(t);const s=e.shape;if("channelsFirst"===this.dataFormat){e=i.p4s(e,[0,2,3,1]);const t=this.size[0]*s[2],n=this.size[1]*s[3],r="nearest"===this.interpolation?i.BHj.resizeNearestNeighbor(e,[t,n]):i.BHj.resizeBilinear(e,[t,n]);return i.p4s(r,[0,3,1,2])}{const t=this.size[0]*s[1],n=this.size[1]*s[2];return"nearest"===this.interpolation?i.BHj.resizeNearestNeighbor(e,[t,n]):i.BHj.resizeBilinear(e,[t,n])}}))}getConfig(){const t={size:this.size,dataFormat:this.dataFormat,interpolation:this.interpolation},e=super.getConfig();return Object.assign(t,e),t}}K.className="UpSampling2D",i.m7h.registerClass(K);class X extends W{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=(0,a.L5)(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=(0,r.Ad)(t.depthwiseConstraint),this.depthwiseRegularizer=I(t.depthwiseRegularizer)}build(t){if((t=(0,m.Wf)(t)).length<4)throw new c.nu(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new c.nu(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const s=t[e],i=[this.kernelSize[0],this.kernelSize[1],s,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",i,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[s*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,i.lub)((()=>{let e=function(t,e,s=[1,1],n="valid",r,a){return(0,i.lub)((()=>{null==r&&(r=(0,R.rf)()),(0,T.cj)(r);let l=j(t,r);if(4!==t.rank)throw new c.nu(`Input for depthwiseConv2d is required to be 4-D, but is instead ${t.rank}-D`);if(4!==e.rank)throw new c.nu(`depthwiseKernel is required to be 4-D, but is instead ${e.rank}-D`);return l=i.B10(l,e,s,"same"===n?"same":"valid","NHWC",a),"channelsFirst"===r&&(l=i.p4s(l,[0,3,1,2])),l}))}(t=(0,m.nQ)(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(e=w.a2(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),e}))}computeOutputShape(t){t=(0,m.Wf)(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],s="channelsFirst"===this.dataFormat?t[3]:t[2],i="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,n=M(e,this.kernelSize[0],this.padding,this.strides[0]),r=M(s,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],i,n,r]:[t[0],n,r,i]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=(0,a.Cx)(this.depthwiseInitializer),t.depthwiseRegularizer=k(this.depthwiseRegularizer),t.depthwiseConstraint=(0,r.xF)(this.depthwiseRegularizer),t}}X.className="DepthwiseConv2D",i.m7h.registerClass(X);var Y=s(41653);function tt(t,e,s,i){if(Array.isArray(t)){if(null!=e||null!=s)throw new c.nu("When inputs is an array, neither initialState or constants should be provided");null!=i&&(s=t.slice(t.length-i,t.length),t=t.slice(0,t.length-i)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function n(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=n(e),constants:s=n(s)}}function et(t,e,s,n=!1,r,a,l=!1,o=!1){return i.lub((()=>{const u=e.shape.length;if(u<3)throw new c.nu(`Input should be at least 3D, but is ${u}D.`);const h=[1,0].concat(E.w6(2,u));if(e=i.p4s(e,h),null!=a)throw new c.nj("The rnn() functoin of the deeplearn.js backend does not support constants yet.");l&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=i.pju(i.pju(r,"bool"),"float32")).rank===u-1&&(r=i.dt4(r,-1)),r=i.p4s(r,h)),n&&(e=i.GYS(e,0),null!=r&&(r=i.GYS(r,0)));const p=[];let d,g=s;const m=e.shape[0],f=i.HHK(e);let b,y;null!=r&&(b=i.HHK(r));for(let e=0;e<m;++e){const s=f[e],n=i.lub((()=>t(s,g)));if(null==r)d=n[0],g=n[1];else{const t=i.lub((()=>{const t=b[e],s=i.luU(i.JpU(t),t),r=i.IHx(i.dC7(n[0],t),i.dC7(g[0],s)),a=g.map(((e,r)=>i.IHx(i.dC7(n[1][r],t),i.dC7(e,s))));return{output:r,newStates:a}}));d=t.output,g=t.newStates}o&&p.push(d)}if(o){const t=1;y=i.knu(p,t)}return[d,y,g]}))}class st extends o.mh{constructor(t){let e;if(super(t),null==t.cell)throw new c.nu("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new ht({cells:t.cell}):t.cell,null==e.stateSize)throw new c.nu("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new o.Zg({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;return E.w6(0,t).map((t=>null))}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){(0,m.XO)(t)&&(t=t[0]);let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const s=e[0];let i;if(i=this.returnSequences?[t[0],t[1],s]:[t[0],s],this.returnState){const s=[];for(const i of e)s.push([t[0],i]);return[i].concat(s)}return i}computeMask(t,e){return i.lub((()=>{Array.isArray(e)&&(e=e[0]);const t=this.returnSequences?e:null;if(this.returnState){const e=this.states.map((t=>null));return[t].concat(e)}return t}))}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let s=0;s<t;++s)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new c.nj("Constants support is not implemented in RNN yet.");(0,m.XO)(t)&&(t=t[0]);const e=this.stateful?t[0]:null,s=t.slice(2);this.inputSpec[0]=new o.Zg({shape:[e,null,...s]});const n=[t[0]].concat(t.slice(2));let r;if(this.cell.build(n),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!i.D5U.arraysEqual(this.stateSpec.map((t=>t.shape[t.shape.length-1])),r))throw new c.nu(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=r.map((t=>new o.Zg({shape:[null,t]})));this.stateful&&this.resetStates()}resetStates(t,e=!1){(0,i.lub)((()=>{if(!this.stateful)throw new c.j1("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape[0];if(null==s)throw new c.nu("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>i.lls([s,t]))):this.states_=[i.lls([s,this.cell.stateSize])];else if(null==t)i.B90(this.states_),null!=this.keptStates&&(i.B90(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>i.lls([s,t]))):this.states_[0]=i.lls([s,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new c.nu(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===e?this.keptStates.push(this.states_.slice()):i.B90(this.states_);for(let e=0;e<this.states_.length;++e){const n=t[e],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[e]:this.cell.stateSize,a=[s,r];if(!i.D5U.arraysEqual(n.shape,a))throw new c.nu(`State ${e} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${n.shape}`);this.states_[e]=n}}this.states_=this.states_.map((t=>i.CnY(t.clone())))}))}apply(t,e){let s=null==e?null:e.initialState,i=null==e?null:e.constants;null==e&&(e={});const n=tt(t,s,i,this.numConstants);t=n.inputs,s=n.initialState,i=n.constants;let r=[],a=[];if(null!=s){e.initialState=s,r=r.concat(s),this.stateSpec=[];for(const t of s)this.stateSpec.push(new o.Zg({shape:t.shape}));a=a.concat(this.stateSpec)}if(null!=i&&(e.constants=i,r=r.concat(i),this.numConstants=i.length),r[0]instanceof o.Iy){const s=[t].concat(r),i=this.inputSpec.concat(a),n=this.inputSpec;this.inputSpec=i;const l=super.apply(s,e);return this.inputSpec=n,l}return super.apply(t,e)}call(t,e){return(0,i.lub)((()=>{const s=null==e?null:e.mask,i=null==e?null:e.training;let n=null==e?null:e.initialState;t=(0,m.nQ)(t),null==n&&(n=this.stateful?this.states_:this.getInitialState(t));const r=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(n.length!==r)throw new c.nu(`RNN Layer has ${r} state(s) but was passed ${n.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const a={training:i},l=et(((t,e)=>{const s=this.cell.call([t].concat(e),a);return[s[0],s.slice(1)]}),t,n,this.goBackwards,s,null,this.unroll,this.returnSequences),o=l[0],u=l[1],h=l[2];this.stateful&&this.resetStates(h,i);const p=this.returnSequences?u:o;return this.returnState?[p].concat(h):p}))}getInitialState(t){return(0,i.lub)((()=>{let e=i.lls(t.shape);return e=i.Smz(e,[1,2]),e=w.dt(e),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map((t=>t>1?w.Gg(e,[1,t]):e)):this.cell.stateSize>1?[w.Gg(e,[1,this.cell.stateSize])]:[e]}))}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const s=this.cell.getConfig();return this.getClassName()===st.className&&(e.cell={className:this.cell.getClassName(),config:s}),Object.assign(Object.assign(Object.assign({},s),t),e)}static fromConfig(t,e,s={}){const i=e.cell,n=(0,p.v)(i,s);return new t(Object.assign(e,{cell:n}))}}st.className="RNN",i.m7h.registerClass(st);class it extends o.mh{}class nt extends it{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,(0,d.iQ)(this.units,"units"),this.activation=(0,y.aI)(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,a.L5)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,a.L5)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,a.L5)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=I(t.kernelRegularizer),this.recurrentRegularizer=I(t.recurrentRegularizer),this.biasRegularizer=I(t.biasRegularizer),this.kernelConstraint=(0,r.Ad)(t.kernelConstraint),this.recurrentConstraint=(0,r.Ad)(t.recurrentConstraint),this.biasConstraint=(0,r.Ad)(t.biasConstraint),this.dropout=E.VV([1,E.Fp([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=E.VV([1,E.Fp([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=(0,m.Wf)(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,i.lub)((()=>{if(2!==t.length)throw new c.nu(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let s=t[1];t=t[0];const n=null!=e.training&&e.training;let r;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ct({ones:()=>i.JpU(t),rate:this.dropout,training:n,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ct({ones:()=>i.JpU(s),rate:this.recurrentDropout,training:n,dropoutFunc:this.dropoutFunc}));const a=this.dropoutMask,l=this.recurrentDropoutMask;r=null!=a?w.AK(i.dC7(t,a),this.kernel.read()):w.AK(t,this.kernel.read()),null!=this.bias&&(r=w.a2(r,this.bias.read())),null!=l&&(s=i.dC7(s,l));let o=i.IHx(r,w.AK(s,this.recurrentKernel.read()));return null!=this.activation&&(o=this.activation.apply(o)),[o,o]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,y.GD)(this.activation),useBias:this.useBias,kernelInitializer:(0,a.Cx)(this.kernelInitializer),recurrentInitializer:(0,a.Cx)(this.recurrentInitializer),biasInitializer:(0,a.Cx)(this.biasInitializer),kernelRegularizer:k(this.kernelRegularizer),recurrentRegularizer:k(this.recurrentRegularizer),biasRegularizer:k(this.biasRegularizer),activityRegularizer:k(this.activityRegularizer),kernelConstraint:(0,r.xF)(this.kernelConstraint),recurrentConstraint:(0,r.xF)(this.recurrentConstraint),biasConstraint:(0,r.xF)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign(Object.assign({},t),e)}}nt.className="SimpleRNNCell",i.m7h.registerClass(nt);class rt extends st{constructor(t){t.cell=new nt(t),super(t)}call(t,e){return(0,i.lub)((()=>{null!=this.cell.dropoutMask&&(i.B90(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(i.B90(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==e?null:e.mask,n=null==e?null:e.training,r=null==e?null:e.initialState;return super.call(t,{mask:s,training:n,initialState:r})}))}static fromConfig(t,e){return new t(e)}}rt.className="SimpleRNN",i.m7h.registerClass(rt);class at extends it{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new c.nu("GRUCell does not support reset_after parameter set to true.");this.units=t.units,(0,d.iQ)(this.units,"units"),this.activation=(0,y.aI)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,y.aI)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,a.L5)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,a.L5)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,a.L5)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=I(t.kernelRegularizer),this.recurrentRegularizer=I(t.recurrentRegularizer),this.biasRegularizer=I(t.biasRegularizer),this.kernelConstraint=(0,r.Ad)(t.kernelConstraint),this.recurrentConstraint=(0,r.Ad)(t.recurrentConstraint),this.biasConstraint=(0,r.Ad)(t.biasConstraint),this.dropout=E.VV([1,E.Fp([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=E.VV([1,E.Fp([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=(0,m.Wf)(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,i.lub)((()=>{if(2!==t.length)throw new c.nu(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);const s=null!=e.training&&e.training;let n=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ct({ones:()=>i.JpU(t),rate:this.dropout,training:s,count:3,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ct({ones:()=>i.JpU(n),rate:this.recurrentDropout,training:s,count:3,dropoutFunc:this.dropoutFunc}));const r=this.dropoutMask,a=this.recurrentDropoutMask;let l,o,u;0<this.dropout&&this.dropout<1&&(t=i.dC7(t,r[0]));let h=w.AK(t,this.kernel.read());this.useBias&&(h=w.a2(h,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(n=i.dC7(n,a[0]));const p=this.recurrentKernel.read(),[d,g]=i.Vl2(p,[2*this.units,this.units],p.rank-1),m=w.AK(n,d),[f,b,y]=i.Vl2(h,3,h.rank-1),[C,S]=i.Vl2(m,2,m.rank-1);l=this.recurrentActivation.apply(i.IHx(f,C)),o=this.recurrentActivation.apply(i.IHx(b,S));const z=w.AK(i.dC7(o,n),g);u=this.activation.apply(i.IHx(y,z));const k=i.IHx(i.dC7(l,n),i.dC7(i.IHx(1,i.W76(l)),u));return[k,k]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,y.GD)(this.activation),recurrentActivation:(0,y.GD)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,a.Cx)(this.kernelInitializer),recurrentInitializer:(0,a.Cx)(this.recurrentInitializer),biasInitializer:(0,a.Cx)(this.biasInitializer),kernelRegularizer:k(this.kernelRegularizer),recurrentRegularizer:k(this.recurrentRegularizer),biasRegularizer:k(this.biasRegularizer),activityRegularizer:k(this.activityRegularizer),kernelConstraint:(0,r.xF)(this.kernelConstraint),recurrentConstraint:(0,r.xF)(this.recurrentConstraint),biasConstraint:(0,r.xF)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign(Object.assign({},t),e)}}at.className="GRUCell",i.m7h.registerClass(at);class lt extends st{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new at(t),super(t)}call(t,e){return(0,i.lub)((()=>{null!=this.cell.dropoutMask&&(i.B90(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(i.B90(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==e?null:e.mask,n=null==e?null:e.training,r=null==e?null:e.initialState;return super.call(t,{mask:s,training:n,initialState:r})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}lt.className="GRU",i.m7h.registerClass(lt);class ot extends it{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,(0,d.iQ)(this.units,"units"),this.activation=(0,y.aI)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,y.aI)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,a.L5)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,a.L5)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,a.L5)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=I(t.kernelRegularizer),this.recurrentRegularizer=I(t.recurrentRegularizer),this.biasRegularizer=I(t.biasRegularizer),this.kernelConstraint=(0,r.Ad)(t.kernelConstraint),this.recurrentConstraint=(0,r.Ad)(t.recurrentConstraint),this.biasConstraint=(0,r.Ad)(t.biasConstraint),this.dropout=E.VV([1,E.Fp([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=E.VV([1,E.Fp([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const s=(t=(0,m.Wf)(t))[t.length-1];let i;if(this.kernel=this.addWeight("kernel",[s,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,s=this.units;i=new((e=class extends a.m7{apply(e,i){const n=t.apply([s]),r=(new a.M6).apply([s]),l=t.apply([2*s]);return w.GZ(w.GZ(n,r),l)}}).className="CustomInit",e)}else i=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,i,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,e){return(0,i.lub)((()=>{const s=null!=e.training&&e.training;if(3!==t.length)throw new c.nu(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let n=t[1];const r=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ct({ones:()=>i.JpU(t),rate:this.dropout,training:s,count:4,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ct({ones:()=>i.JpU(n),rate:this.recurrentDropout,training:s,count:4,dropoutFunc:this.dropoutFunc}));const a=this.dropoutMask,l=this.recurrentDropoutMask;let o,u,h,p;0<this.dropout&&this.dropout<1&&(t=i.dC7(t,a[0]));let d=w.AK(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(n=i.dC7(n,l[0])),d=i.IHx(d,w.AK(n,this.recurrentKernel.read())),this.useBias&&(d=w.a2(d,this.bias.read()));const[g,m,f,b]=i.Vl2(d,4,d.rank-1);o=this.recurrentActivation.apply(g),u=this.recurrentActivation.apply(m),h=i.IHx(i.dC7(u,r),i.dC7(o,this.activation.apply(f))),p=this.recurrentActivation.apply(b);const y=i.dC7(p,this.activation.apply(h));return[y,y,h]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,y.GD)(this.activation),recurrentActivation:(0,y.GD)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,a.Cx)(this.kernelInitializer),recurrentInitializer:(0,a.Cx)(this.recurrentInitializer),biasInitializer:(0,a.Cx)(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:k(this.kernelRegularizer),recurrentRegularizer:k(this.recurrentRegularizer),biasRegularizer:k(this.biasRegularizer),activityRegularizer:k(this.activityRegularizer),kernelConstraint:(0,r.xF)(this.kernelConstraint),recurrentConstraint:(0,r.xF)(this.recurrentConstraint),biasConstraint:(0,r.xF)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign(Object.assign({},t),e)}}ot.className="LSTMCell",i.m7h.registerClass(ot);class ut extends st{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new ot(t),super(t)}call(t,e){return(0,i.lub)((()=>{null!=this.cell.dropoutMask&&(i.B90(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(i.B90(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==e?null:e.mask,n=null==e?null:e.training,r=null==e?null:e.initialState;return super.call(t,{mask:s,training:n,initialState:r})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}ut.className="LSTM",i.m7h.registerClass(ut);class ht extends it{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,e){return(0,i.lub)((()=>{let s=t.slice(1);const i=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?i.push(s.splice(0,t.stateSize.length)):i.push(s.splice(0,1));i.reverse();const n=[];let r;for(let a=0;a<this.cells.length;++a){const l=this.cells[a];s=i[a],r=0===a?[t[0]].concat(s):[r[0]].concat(s),r=l.call(r,e),n.push(r.slice(1))}s=[];for(const t of n.slice().reverse())s.push(...t);return[r[0]].concat(s)}))}build(t){let e;(0,m.XO)(t)&&(t=t[0]),this.cells.forEach(((s,i)=>{(0,T.f4)(`RNNCell_${i}`,(()=>{s.build(t),e=Array.isArray(s.stateSize)?s.stateSize[0]:s.stateSize,t=[t[0],e]}))})),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map((t=>({className:t.getClassName(),config:t.getConfig()})))};return Object.assign(Object.assign({},t),e)}static fromConfig(t,e,s={}){const i=[];for(const t of e.cells)i.push((0,p.v)(t,s));return new t({cells:i})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return(0,Y.FQ)(t)}setWeights(t){const e=[];for(const s of this.cells){const i=s.weights.length,n=t.splice(i);for(let t=0;t<s.weights.length;++t)e.push([s.weights[t],n[t]])}(0,Y.zb)(e)}}function ct(t){const{ones:e,rate:s,training:n=!1,count:r=1,dropoutFunc:a}=t,l=()=>null!=a?a(e(),s):w.rv(e(),s),o=()=>w.KC(l,e,n);return!r||r<=1?i.CnY(o().clone()):Array(r).fill(void 0).map(o).map((t=>i.CnY(t.clone())))}ht.className="StackedRNNCells",i.m7h.registerClass(ht);class pt extends st{constructor(t){if(t.unroll)throw new c.nj("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new c.nj("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new o.Zg({ndim:5})]}call(t,e){return i.lub((()=>{if(null!=this.cell.dropoutMask&&(i.B90(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(i.B90(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),e&&e.constants)throw new c.nu("ConvRNN2D cell does not support constants");const s=null==e?null:e.mask,n=null==e?null:e.training,r=null==e?null:e.initialState;return super.call(t,{mask:s,training:n,initialState:r})}))}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return i.lub((()=>{const{stateSize:e}=this.cell,s=t.shape,n=this.computeSingleOutputShape(s),r=[n[0],...n.slice(2)],a=i.lls(r);return Array.isArray(e)?Array(e.length).fill(a):[a]}))}resetStates(t,e=!1){i.lub((()=>{if(!this.stateful)throw new c.j1("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape,n=this.computeSingleOutputShape(s),r=[n[0],...n.slice(2)];if(null==s[0])throw new c.nu("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>i.lls(r))):this.states_=[i.lls(r)];else if(null==t)i.B90(this.states_),null!=this.keptStates&&(i.B90(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>i.lls(r))):this.states_[0]=i.lls(r);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new c.nu(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);e?this.keptStates.push(this.states_.slice()):i.B90(this.states_);for(let e=0;e<this.states_.length;++e){const s=t[e],n=r;if(!i.D5U.arraysEqual(s.shape,n))throw new c.nu(`State ${e} is incompatible with layer ${this.name}: expected shape=${n}, received shape=${s.shape}`);this.states_[e]=s}}this.states_=this.states_.map((t=>i.CnY(t.clone())))}))}computeSingleOutputShape(t){const{dataFormat:e,filters:s,kernelSize:i,padding:n,strides:r,dilationRate:a}=this.cell,l="channelsFirst"===e,o=t[l?3:2],u=t[l?4:3],h=M(o,i[0],n,r[0],a[0]),c=M(u,i[1],n,r[1],a[1]);return[...t.slice(0,2),...l?[s,h,c]:[h,c,s]]}}pt.className="ConvRNN2D";class dt extends ot{constructor(t){const{filters:e,kernelSize:s,strides:i,padding:n,dataFormat:r,dilationRate:a}=t;super(Object.assign(Object.assign({},t),{units:e})),this.filters=e,(0,d.iQ)(this.filters,"filters"),this.kernelSize=O(s,2,"kernelSize"),this.kernelSize.forEach((t=>(0,d.iQ)(t,"kernelSize"))),this.strides=O(i||1,2,"strides"),this.strides.forEach((t=>(0,d.iQ)(t,"strides"))),this.padding=n||"valid",(0,T.zb)(this.padding),this.dataFormat=r||"channelsLast",(0,T.cj)(this.dataFormat),this.dilationRate=O(a||1,2,"dilationRate"),this.dilationRate.forEach((t=>(0,d.iQ)(t,"dilationRate")))}build(t){var e;t=(0,m.Wf)(t);const s="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[s])throw new c.nu(`The channel dimension of the input should be defined. Found ${t[s]}`);const n=t[s],r=this.kernelSize.concat([n,4*this.filters]);this.kernel=this.addWeight("kernel",r,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const l=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",l,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const s=this.biasInitializer,n=this.filters;t=new((e=class extends a.m7{apply(t,e){const r=s.apply([n]),a=i.iUs([n]),l=s.apply([2*n]);return w.mV([r,a,l])}}).className="CustomInit",e)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,e){return i.lub((()=>{if(3!==t.length)throw new c.nu(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);const s=e.training||!1,n=t[0],r=t[1],a=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ct({ones:()=>i.JpU(n),rate:this.dropout,training:s,count:4,dropoutFunc:this.dropoutFunc}));const l=this.dropoutMask,o=(t,e,s)=>e&&e[s]?i.dC7(e[s],t):t;let u=o(n,l,0),h=o(n,l,1),p=o(n,l,2),d=o(n,l,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ct({ones:()=>i.JpU(r),rate:this.recurrentDropout,training:s,count:4,dropoutFunc:this.dropoutFunc}));const g=this.recurrentDropoutMask;let m=o(r,g,0),f=o(r,g,1),b=o(r,g,2),y=o(r,g,3);const[w,C,S,z]=i.Vl2(this.kernel.read(),4,3),[k,A,I,v]=this.useBias?i.Vl2(this.bias.read(),4):[null,null,null,null];u=this.inputConv(u,w,k,this.padding),h=this.inputConv(h,C,A,this.padding),p=this.inputConv(p,S,I,this.padding),d=this.inputConv(d,z,v,this.padding);const[N,x,F,L]=i.Vl2(this.recurrentKernel.read(),4,3);m=this.recurrentConv(m,N),f=this.recurrentConv(f,x),b=this.recurrentConv(b,F),y=this.recurrentConv(y,L);const D=this.recurrentActivation.apply(i.IHx(u,m)),R=this.recurrentActivation.apply(i.IHx(h,f)),T=i.IHx(i.dC7(R,a),i.dC7(D,this.activation.apply(i.IHx(p,b)))),E=i.dC7(this.recurrentActivation.apply(i.IHx(d,y)),this.activation.apply(T));return[E,E,T]}))}getConfig(){const t=super.getConfig(),{units:e}=t,s=function(t,e){var s={};for(var i in t)Object.prototype.hasOwnProperty.call(t,i)&&e.indexOf(i)<0&&(s[i]=t[i]);if(null!=t&&"function"==typeof Object.getOwnPropertySymbols){var n=0;for(i=Object.getOwnPropertySymbols(t);n<i.length;n++)e.indexOf(i[n])<0&&Object.prototype.propertyIsEnumerable.call(t,i[n])&&(s[i[n]]=t[i[n]])}return s}(t,["units"]),i={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign(Object.assign({},s),i)}inputConv(t,e,s,n){const r=i.Tek(t,e,this.strides,n||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return s?w.a2(r,s,this.dataFormat):r}recurrentConv(t,e){return i.Tek(t,e,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}dt.className="ConvLSTM2DCell",i.m7h.registerClass(dt);class gt extends pt{constructor(t){const e=new dt(t);super(Object.assign(Object.assign({},t),{cell:e}))}static fromConfig(t,e){return new t(e)}}gt.className="ConvLSTM2D",i.m7h.registerClass(gt);class mt extends o.mh{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,s=[];for(let t=0;t<this.noiseShape.length;++t)s.push(null==this.noiseShape[t]?e[t]:this.noiseShape[t]);return s}call(t,e){return(0,i.lub)((()=>{this.invokeCallHook(t,e);const s=(0,m.nQ)(t);if(0<this.rate&&this.rate<1){const t=null!=e.training&&e.training,i=this.getNoiseShape(s);return w.KC((()=>w.rv(s,this.rate,i,this.seed)),(()=>s),t)}return t}))}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}mt.className="Dropout",i.m7h.registerClass(mt);class ft extends mt{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}ft.className="SpatialDropout1D",i.m7h.registerClass(ft);class bt extends o.mh{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,(0,d.iQ)(this.units,"units"),this.activation=(0,y.aI)(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=(0,a.L5)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=(0,a.L5)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=(0,r.Ad)(t.kernelConstraint),this.biasConstraint=(0,r.Ad)(t.biasConstraint),this.kernelRegularizer=I(t.kernelRegularizer),this.biasRegularizer=I(t.biasRegularizer),this.activityRegularizer=I(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=(0,m.Wf)(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=(0,m.Wf)(t)).slice();return e[e.length-1]=this.units,e}call(t,e){return(0,i.lub)((()=>{this.invokeCallHook(t,e);const s=(0,m.nQ)(t),i=(0,d.WT)(this.activation.getClassName());let n;return null!=i?n=w.AK(s,this.kernel.read(),i,this.bias?this.bias.read():null):(n=w.AK(s,this.kernel.read()),null!=this.bias&&(n=w.a2(n,this.bias.read())),null!=this.activation&&(n=this.activation.apply(n))),n}))}getConfig(){const t={units:this.units,activation:(0,y.GD)(this.activation),useBias:this.useBias,kernelInitializer:(0,a.Cx)(this.kernelInitializer),biasInitializer:(0,a.Cx)(this.biasInitializer),kernelRegularizer:k(this.kernelRegularizer),biasRegularizer:k(this.biasRegularizer),activityRegularizer:k(this.activityRegularizer),kernelConstraint:(0,r.xF)(this.kernelConstraint),biasConstraint:(0,r.xF)(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}bt.className="Dense",i.m7h.registerClass(bt);class yt extends o.mh{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=(0,m.Wf)(t);for(const e of t.slice(1))if(null==e)throw new c.nu(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],(0,E.NS)(t,1)]}call(t,e){return(0,i.lub)((()=>{this.invokeCallHook(t,e);let s=(0,m.nQ)(t);if("channelsFirst"===this.dataFormat&&s.rank>1){const t=[0];for(let e=2;e<s.rank;++e)t.push(e);t.push(1),s=(0,i.p4s)(s,t)}return w.Uz(s)}))}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}yt.className="Flatten",i.m7h.registerClass(yt);class wt extends o.mh{constructor(t){super(t),this.supportsMasking=!0,this.activation=(0,y.aI)(t.activation)}call(t,e){return(0,i.lub)((()=>{this.invokeCallHook(t,e);const s=(0,m.nQ)(t);return this.activation.apply(s)}))}getConfig(){const t={activation:(0,y.GD)(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}wt.className="Activation",i.m7h.registerClass(wt);class Ct extends o.mh{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,e){return(0,i.lub)((()=>(t=(0,m.nQ)(t),w.rx(t,this.n))))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}Ct.className="RepeatVector",i.m7h.registerClass(Ct);class St extends o.mh{constructor(t){super(t),this.targetShape=t.targetShape;for(let t=0;t<this.targetShape.length;++t)this.isUnknown(this.targetShape[t])&&(this.targetShape[t]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const s="Total size of new array must be unchanged.",i=e.slice();let n=1,r=null;for(let t=0;t<i.length;++t){const e=i[t];if(this.isUnknown(e)){if(null!==r)throw new c.nu("Can only specifiy one unknown dimension.");r=t}else n*=e}const a=(0,E.NS)(t);if(null!==r){if(0===n||a%n!=0)throw new c.nu(s);i[r]=a/n}else if(a!==n)throw new c.nu(s);return i}computeOutputShape(t){let e=!1;for(let s=0;s<t.length;++s)if(this.isUnknown(t[s])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,e){return(0,i.lub)((()=>{this.invokeCallHook(t,e);const s=(0,m.nQ)(t),n=s.shape,r=n.slice(0,1).concat(this.fixUnknownDimension(n.slice(1),this.targetShape));return(0,i.XLQ)(s,r)}))}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}St.className="Reshape",i.m7h.registerClass(St);class zt extends o.mh{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${t.dims} instead.`);const e=(0,E.w6)(1,t.dims.length+1);if(!i.D5U.arraysEqual(t.dims.slice().sort(),e))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new o.Zg({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=(0,m.Wf)(t)).slice();return this.dims.forEach(((s,i)=>{e[i+1]=t[s]})),e}call(t,e){return(0,i.p4s)((0,m.nQ)(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}zt.className="Permute",i.m7h.registerClass(zt);class kt extends o.mh{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,e){const s=(0,m.nQ)(t);return(0,i.YjB)((0,i.Quu)(s,this.maskValue),-1)}call(t,e){return(0,i.lub)((()=>{this.invokeCallHook(t,e);const s=(0,m.nQ)(t),n=(0,i.YjB)((0,i.Quu)(s,this.maskValue),-1,!0);return(0,i.dC7)(s,(0,i.pju)(n,s.dtype))}))}}kt.className="Masking",i.m7h.registerClass(kt);class At extends o.mh{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(d.zZ(t.inputLength))}this.inputDim=t.inputDim,d.iQ(this.inputDim,"inputDim"),this.outputDim=t.outputDim,d.iQ(this.outputDim,"outputDim"),this.embeddingsInitializer=(0,a.L5)(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=I(t.embeddingsRegularizer),this.activityRegularizer=I(t.activityRegularizer),this.embeddingsConstraint=(0,r.Ad)(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,e){return(0,i.lub)((()=>this.maskZero?(t=(0,m.nQ)(t),(0,i.Quu)(t,(0,i.P84)(t))):null))}computeOutputShape(t){if(t=(0,m.Wf)(t),null==this.inputLength)return[...t,this.outputDim];const e=d.zZ(this.inputLength);if(e.length!==t.length-1)throw new c.nu(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);{let s=0;for(let i=0;i<e.length;++i){const n=e[i],r=t[i+1];if(null!=n&&null!=r&&n!==r)throw new c.nu(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);null==n&&(e[s]=r),s++}}return[t[0],...e,this.outputDim]}call(t,e){return(0,i.lub)((()=>{this.invokeCallHook(t,e);let s=(0,m.nQ)(t);"int32"!==s.dtype&&(s=w.pj(s,"int32"));const n=w.Iq(this.embeddings.read(),(0,i.XLQ)(s,[s.size]));return(0,i.XLQ)(n,(0,m.Wf)(this.computeOutputShape(s.shape)))}))}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:(0,a.Cx)(this.embeddingsInitializer),embeddingsRegularizer:k(this.embeddingsRegularizer),activityRegularizer:k(this.activityRegularizer),embeddingsConstraint:(0,r.xF)(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}At.className="Embedding",i.m7h.registerClass(At);var It=s(86275);class vt extends o.mh{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new c.nj}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const s=t.slice(0,t.length-e.length);for(let i=0;i<e.length;++i){const n=t[t.length-e.length+i],r=e[i];if(null==n||null==r||n<0||r<0)s.push(null);else if(1===n)s.push(r);else if(1===r)s.push(n);else{if(n!==r)throw new c.nu("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));s.push(n)}}return s}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[(0,m.Wf)(t)]),t.length<2)throw new c.nu(`A merge layer should be called on an Array of at least 2 inputs. Got ${t.length} input(s).`);let e=[];for(const s of t)null!=s&&null!==s[0]&&e.push(s[0]);if(e=d.Tw(e),e.length>1)throw new c.nu(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(t)}.`);let s=null==t[0]?null:t[0].slice(1);for(let e=1;e<t.length;++e){const i=null==t[e]?null:t[e].slice(1);s=this.computeElementwiseOpOutputShape(s,i)}const i=t.map((t=>t.length));-1===t.indexOf(null)&&1===d.Tw(i).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,e){return(0,i.lub)((()=>{if(this.reshapeRequired){const e=[],s=t.map((t=>t.rank));if(-1===s.indexOf(null)){const i=E.Fp(s);for(let s of t){const t=s.rank;for(let e=0;e<i-t;++e)s=w.dt(s,1);e.push(s)}return this.mergeFunction(e)}{let s=!1;for(const n of t){const t=n.rank;if(null==t){const t=n.shape,r=t[0],a=t.slice(1).concat([r]);let l=i.XLQ(n,[r].concat(E.NS(t.slice(1))));l=i.p4s(l,[1,0]),l=i.XLQ(l,a),e.push(l),s=!0}else if(t>1){const r=E.w6(1,t).concat([0]);e.push(i.p4s(n,r)),s=!0}else e.push(n)}let n=this.mergeFunction(e);const r=n.rank;if(s)if(null==r){const t=n.shape,e=t[t.length-1],s=[e].concat(t.slice(0,t.length-1));n=i.XLQ(i.p4s(i.XLQ(n,[-1,e]),[1,0]),s)}else if(r>1){const t=[r-1].concat(E.w6(0,r-1));n=i.p4s(n,t)}return n}}return this.mergeFunction(t)}))}computeOutputShape(t){let e;e=null==t[0]?null:t[0].slice(1);for(let s=1;s<t.length;++s){const i=null==t[s]?null:t[s].slice(1);e=this.computeElementwiseOpOutputShape(e,i)}let s=[];for(const e of t)null!=e&&null!==e[0]&&s.push(e[0]);return s=d.Tw(s),e=1===s.length?s.concat(e):[null].concat(e),e}computeMask(t,e){return i.lub((()=>{if(null==e)return null;if(!Array.isArray(e))throw new c.nu("`mask` should be an Array");if(!Array.isArray(t))throw new c.nu("`inputs` should be an Array");if(e.length!==t.length)throw new c.nu(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${t.length} vs ${e.length})`);if(e.every((t=>null==t)))return null;let s=(e=e.map((t=>null==t?t:i.dt4(t,0))))[0];for(let t=1;t<e.length-1;++t)s=i.HvI(s,e[t]);return s}))}}class Nt extends vt{constructor(t){super(t)}mergeFunction(t){return(0,i.lub)((()=>{let e=t[0].clone();for(let s=1;s<t.length;++s)e=i.IHx(e,t[s]);return e}))}}Nt.className="Add",i.m7h.registerClass(Nt);class xt extends vt{constructor(t){super(t)}mergeFunction(t){return(0,i.lub)((()=>{let e=t[0].clone();for(let s=1;s<t.length;++s)e=i.dC7(e,t[s]);return e}))}}xt.className="Multiply",i.m7h.registerClass(xt);class Ft extends vt{constructor(t){super(t)}mergeFunction(t){return(0,i.lub)((()=>{let e=t[0].clone();for(let s=1;s<t.length;++s)e=i.IHx(e,t[s]);return i.dC7(1/t.length,e)}))}}Ft.className="Average",i.m7h.registerClass(Ft);class Lt extends vt{constructor(t){super(t)}mergeFunction(t){return(0,i.lub)((()=>{let e=t[0];for(let s=1;s<t.length;++s)e=i.gWQ(e,t[s]);return e}))}}Lt.className="Maximum",i.m7h.registerClass(Lt);class Dt extends vt{constructor(t){super(t)}mergeFunction(t){return(0,i.lub)((()=>{let e=t[0];for(let s=1;s<t.length;++s)e=i.LTh(e,t[s]);return e}))}}Dt.className="Minimum",i.m7h.registerClass(Dt);class Rt extends vt{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new c.nu("A `Concatenate` layer should be called on a list of at least 2 inputs");let e=!0;for(const s of t)if(null!=s){e=!1;break}if(e)return;const s=[];for(let e=0;e<t.length;++e){const n=t[e].slice();n.splice(this.axis,1);let r=!1;for(const t of s)if(i.D5U.arraysEqual(t,n)){r=!0;break}r||s.push(n)}if(s.length>1)throw new c.nu("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return(0,i.lub)((()=>w.mV(t,this.axis)))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new c.nu("A `Concatenate` layer should be called on a list of inputs.");const e=t,s=e[0].slice(),i=this.axis<0?s.length+this.axis:this.axis;for(const t of e.slice(1)){if(null==s[i]||null==t[i]){s[i]=null;break}s[i]+=t[i]}return s}computeMask(t,e){if(null==e)return null;if(!Array.isArray(e))throw new c.nu("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new c.nu("`inputs` should be an array for Concatenate");if(e.length!==t.length)throw new c.nu(`Mismatch in the length of mask (${e.length}) and the legnth of inputs (${t.length})`);return i.lub((()=>{let s=!0;if(e.forEach((t=>{null==t||(s=!1)})),s)return null;const n=[];for(let s=0;s<t.length;++s)null==e[s]?n.push(i.pju(i.JpU(t[s]),"bool")):e[s].rank<t[s].rank?n.push(i.dt4(e[s],-1)):n.push(e[s]);const r=i.zoF(n,this.axis);return i.$6P(r,-1,!1)}))}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function Tt(t,e){for(;t<0;)t+=e;return t}Rt.className="Concatenate",i.m7h.registerClass(Rt);class Et extends vt{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){i.D5U.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const e=t[0],s=t[1];if(e.length>3||s.length>3)throw new c.nj("Dot layer does not support tensors of 4D or higher rank yet.");const n=this.interpretAxes(e,s);if(e[n[0]]!==s[n[1]])throw new c.nu(`Dimension incompatibility: ${e[n[0]]} !== ${s[n[1]]}`)}mergeFunction(t){if(2!==t.length)throw new c.nu(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let e,s=t[0],n=t[1];return e=Array.isArray(this.axes)?this.axes.map(((e,s)=>Tt(e,t[s].shape.length))):[Tt(this.axes,s.shape.length),Tt(this.axes,n.shape.length)],this.normalize&&(s=(0,It.Eq)(s,e[0]),n=(0,It.Eq)(n,e[1])),function(t,e,s){if(t.shape.length>3||e.shape.length>3)throw new c.nj("batchDot is not implemented for tensors of 4D or higher rank yet");if(i.D5U.assert(t.shape.length>=2,(()=>`batchDot requires the rank of x to be >= 2, but got ${t.shape.length}`)),i.D5U.assert(t.shape.length>=2,(()=>`batchDot requires the rank of y to be >= 2, but got ${e.shape.length}`)),"number"==typeof s&&(s=[s,s]),"complex64"===t.dtype||"complex64"===e.dtype)throw new c.nj("batchDot is not implemented for complex64-type Tensors yet.");const n=t.shape.length,r=e.shape.length;null==s&&(s=[n-1,r-2]);const a=s;return i.lub((()=>{let s,l;if(n>r){s=n-r;const t=[];for(let e=0;e<s;++e)t.push(1);e=i.XLQ(e,e.shape.concat(t))}else if(r>n){s=r-n;const e=[];for(let t=0;t<s;++t)e.push(1);t=i.XLQ(t,t.shape.concat(e))}else s=0;if(2===t.shape.length&&2===e.shape.length)l=a[0]===a[1]?i.Smz(i.dC7(t,e),a[0]):i.Smz(i.dC7(i.p4s(t,[1,0]),e),a[1]);else{const s=a[0]!==t.shape.length-1,n=a[1]===e.shape.length-1;l=i.OI3(t,e,s,n)}if(s>0){let t;t=n>r?n+r-3:n-1;const e=[];for(let i=t;i<t+s;++i)e.push(i);l=i.L9e(l,e)}return 1===l.shape.length&&(l=i.dt4(l,1)),l}))}(s,n,e)}interpretAxes(t,e){let s;return s=Array.isArray(this.axes)?this.axes:[Tt(this.axes,t.length),Tt(this.axes,e.length)],s}computeOutputShape(t){i.D5U.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const e=t[0].slice(),s=t[1].slice();if(e.length>3||s.length>3)throw new c.nj("Dot layer does not support tensors of 4D or higher rank yet.");const n=this.interpretAxes(e,s);e.splice(n[0],1),s.splice(n[1],1),s.splice(0,1);const r=e.concat(s);return 1===r.length&&r.push(1),r}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}Et.className="Dot",i.m7h.registerClass(Et);class Ot extends o.mh{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,e){return(0,i.lub)((()=>{this.invokeCallHook(t,e);const s=(0,m.nQ)(t);return w.KC((()=>(0,i.IHx)(w.nG(s.shape,0,this.stddev),s)),(()=>s),e.training||!1)}))}}Ot.className="GaussianNoise",i.m7h.registerClass(Ot);class Mt extends o.mh{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return(0,i.lub)((()=>{this.invokeCallHook(t,e);const s=(0,m.nQ)(t);if(this.rate>0&&this.rate<1){const t=()=>{const t=Math.sqrt(this.rate/(1-this.rate));return(0,i.dC7)(s,w.nG(s.shape,1,t))};return w.KC(t,(()=>s),e.training||!1)}return s}))}}Mt.className="GaussianDropout",i.m7h.registerClass(Mt);class Ut extends o.mh{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||(0,m.nQ)(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return(0,i.lub)((()=>{if(this.rate<1&&this.rate>0){const s=this._getNoiseShape(t),n=()=>{const e=(0,m.nQ)(t),n=-1.7580993408473766;let r=(0,i.brS)((0,i.LGj)(s),this.rate);r=w.pj(r,"float32");const a=((1-this.rate)*(1+this.rate*n**2))**-.5,l=-a*n*this.rate,o=(0,i.IHx)((0,i.dC7)(e,r),(0,i.dC7)((0,i.IHx)(r,-1),n));return(0,i.IHx)((0,i.dC7)(o,a),l)};return w.KC(n,(()=>(0,m.nQ)(t)),e.training||!1)}return t}))}}function jt(t,e,s,n,r,a=.001){let l;if(2===t.rank)l=i.Dxk(t,e,s,n,r,a);else if(3===t.rank)l=i.JY5(t,e,s,n,r,a);else{if(4!==t.rank)throw new c.nj(`batchNormalization is not implemented for array of rank ${t.rank} yet`);l=i.p3b(t,e,s,n,r,a)}return l}Ut.className="AlphaDropout",i.m7h.registerClass(Ut);class _t extends o.mh{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=(0,a.L5)(t.betaInitializer||"zeros"),this.gammaInitializer=(0,a.L5)(t.gammaInitializer||"ones"),this.movingMeanInitializer=(0,a.L5)(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=(0,a.L5)(t.movingVarianceInitializer||"ones"),this.betaConstraint=(0,r.Ad)(t.betaConstraint),this.gammaConstraint=(0,r.Ad)(t.gammaConstraint),this.betaRegularizer=I(t.betaRegularizer),this.gammaRegularizer=I(t.gammaRegularizer)}build(t){t=(0,m.Wf)(t);const e=this.axis>=0?this.axis:this.axis+t.length,s=t[e];if(null==s)throw new c.nu(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(t)}.`);this.inputSpec=[new o.Zg({ndim:t.length,axes:{[e]:s}})];const i=[s];this.scale&&(this.gamma=this.addWeight("gamma",i,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",i,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",i,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",i,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,e){return(0,i.lub)((()=>{const s=null!=e.training&&e.training,n=(0,m.nQ)(t),r=n.shape,a=r.length,l=E.w6(0,a),o=this.axis>=0?this.axis:this.axis+a;l.splice(o,1);const u=d.JE(1,a);u[o]=r[o];const h=l.slice();h.sort();const c=!i.D5U.arraysEqual(h,E.w6(0,a).slice(0,a-1));if(!s)return(()=>{if(c){const t=(0,i.XLQ)(this.movingMean.read(),u),e=(0,i.XLQ)(this.movingVariance.read(),u),s=this.center?(0,i.XLQ)(this.beta.read(),u):null,r=this.scale?(0,i.XLQ)(this.gamma.read(),u):null;return jt(n,t,e,s,r,this.epsilon)}return jt(n,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[p,g,f]=function(t,e,s,n,r=.001){return i.D5U.arraysEqual(n.slice().sort(),E.w6(0,t.rank-1))?function(t,e,s,n,r=.001){return(0,i.lub)((()=>{const a=i.Gi7(t,n),l=a.mean,o=a.variance;return[jt(t,l,o,s,e,r),l,o]}))}(t,e,s,n,r):function(t,e,s,n,r=.001){return(0,i.lub)((()=>{const a=i.Gi7(t,n),l=a.mean,o=a.variance,u=[];for(const e of E.w6(0,t.rank))-1!==n.indexOf(e)?u.push(1):u.push(t.shape[e]);const h=(0,i.XLQ)(l,u),c=(0,i.XLQ)(o,u),p=null==e?null:(0,i.XLQ)(e,u),d=null==s?null:(0,i.XLQ)(s,u);return[jt(t,h,c,d,p,r),l,o]}))}(t,e,s,n,r)}(n,this.gamma.read(),this.beta.read(),l,this.epsilon),b=(t,e,s)=>{i.lub((()=>{const n=1-s,r=t.read(),a=i.dC7(i.luU(r,e),n);t.write(i.luU(r,a))}))};return(()=>{b(this.movingMean,g,this.momentum),b(this.movingVariance,f,this.momentum)})(),p}))}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:(0,a.Cx)(this.betaInitializer),gammaInitializer:(0,a.Cx)(this.gammaInitializer),movingMeanInitializer:(0,a.Cx)(this.movingMeanInitializer),movingVarianceInitializer:(0,a.Cx)(this.movingVarianceInitializer),betaRegularizer:k(this.betaRegularizer),gammaRegularizer:k(this.gammaRegularizer),betaConstraint:(0,r.xF)(this.betaConstraint),gammaConstraint:(0,r.xF)(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}_t.className="BatchNormalization",i.m7h.registerClass(_t);class $t extends o.mh{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const t of this.axis)if(!Number.isInteger(t))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=(0,a.L5)(t.betaInitializer||"zeros"),this.gammaInitializer=(0,a.L5)(t.gammaInitializer||"ones"),this.betaRegularizer=I(t.betaRegularizer),this.gammaRegularizer=I(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=(0,m.Wf)(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let t=0;t<this.axis.length;++t)this.axis[t]<0&&(this.axis[t]+=e);for(const t of this.axis)if(t<0||t>=e)throw new Error(`Invalid axis: ${t}`);if(this.axis.length!==d.Tw(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const s=this.axis.map((e=>t[e]));this.scale?this.gamma=this.addWeight("gamma",s,"float32",this.gammaInitializer,this.gammaRegularizer,!0):this.gamma=null,this.center?this.beta=this.addWeight("beta",s,"float32",this.betaInitializer,this.betaRegularizer,!0):this.beta=null,this.built=!0}call(t,e){const s=(0,m.nQ)(t),n=s.shape,r=n.length;return(0,i.lub)((()=>{let{mean:t,variance:e}=(0,i.Gi7)(s,this.axis,!0);const a=d.JE(1,r);for(const t of this.axis)a[t]=n[t];const l=t=>null!=t&&t.shape.length!==r?i.XLQ(t,a):t;let o=this.scale?l(this.gamma.read()):null,u=this.center?l(this.beta.read()):null;const h=[],c=[];for(let t=0;t<r;++t)-1!==this.axis.indexOf(t)?(h.push(n[t]),c.push(1)):(h.push(1),c.push(n[t]));return t=i.Gg6(t,h),e=i.Gg6(e,h),null!=o&&(o=i.Gg6(o,c)),null!=u&&(u=i.Gg6(u,c)),jt(s,t,e,u,o,this.epsilon)}))}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:(0,a.Cx)(this.betaInitializer),gammaInitializer:(0,a.Cx)(this.gammaInitializer),betaRegularizer:k(this.betaRegularizer),gammaRegularizer:k(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}$t.className="LayerNormalization",i.m7h.registerClass($t);class Wt extends o.mh{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?(0,R.rf)():t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new c.nu(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,s;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],s=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new c.nu(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new c.nu(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);s=t.padding[1]}this.padding=[e,s]}this.inputSpec=[new o.Zg({ndim:4})]}computeOutputShape(t){let e,s;return t=(0,m.Wf)(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,s=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,s]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,s=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,s,t[3]])}call(t,e){return(0,i.lub)((()=>{return e=(0,m.nQ)(t),s=this.padding,n=this.dataFormat,(0,i.lub)((()=>{if(4!==e.rank)throw new c.nu(`temporalPadding expects input tensor to be 4-D, but received a ${e.rank}-D tensor.`);if(null==s&&(s=[[1,1],[1,1]]),2!==s.length||2!==s[0].length||2!==s[1].length)throw new c.nu("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==n&&(n=(0,R.rf)()),"channelsLast"!==n&&"channelsFirst"!==n)throw new c.nu(`Unknown data format: ${n}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let t;return t="channelsFirst"===n?[[0,0],[0,0],s[0],s[1]]:[[0,0],s[0],s[1],[0,0]],i.vku(e,t)}));var e,s,n}))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function Bt(t,e,s,n,r,a){return(0,i.lub)((()=>{let l;(0,T.cj)(r),(0,T.Lp)(a),(0,T.zb)(n),null==s&&(s=[1,1]),null==n&&(n="valid"),null==r&&(r=(0,R.rf)()),null==a&&(a="max"),t=j(t,r);const o="same"===n?"same":"valid";return l="max"===a?i._sB(t,e,s,o):i.wS1(t,e,s,o),"channelsFirst"===r&&(l=i.p4s(l,[0,3,1,2])),l}))}function Ht(t,e,s,n,r,a){return(0,i.lub)((()=>{let l;(0,T.cj)(r),(0,T.Lp)(a),(0,T.zb)(n),null==s&&(s=[1,1,1]),null==n&&(n="valid"),null==r&&(r=(0,R.rf)()),null==a&&(a="max"),t=_(t,r);const o="same"===n?"same":"valid";return l="max"===a?i.YQQ(t,e,s,o):i.uR5(t,e,s,o),"channelsFirst"===r&&(l=i.p4s(l,[0,4,1,2,3])),l}))}Wt.className="ZeroPadding2D",i.m7h.registerClass(Wt);class Qt extends o.mh{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"==typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!=typeof t.poolSize[0])throw new c.nu(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.poolSize)}`);this.poolSize=t.poolSize}if((0,d.iQ)(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"==typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!=typeof t.strides[0])throw new c.nu(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.strides)}`);this.strides=t.strides}(0,d.iQ)(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,(0,T.zb)(this.padding),this.inputSpec=[new o.Zg({ndim:3})]}computeOutputShape(t){const e=M((t=(0,m.Wf)(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,e){return(0,i.lub)((()=>{this.invokeCallHook(t,e),t=w.dt((0,m.nQ)(t),2);const s=this.poolingFunction((0,m.nQ)(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return i.L9e(s,[2])}))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class Vt extends Qt{constructor(t){super(t)}poolingFunction(t,e,s,i,n){return(0,T.cj)(n),(0,T.zb)(i),Bt(t,e,s,i,n,"max")}}Vt.className="MaxPooling1D",i.m7h.registerClass(Vt);class Jt extends Qt{constructor(t){super(t)}poolingFunction(t,e,s,i,n){return(0,T.cj)(n),(0,T.zb)(i),Bt(t,e,s,i,n,"avg")}}Jt.className="AveragePooling1D",i.m7h.registerClass(Jt);class Zt extends o.mh{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new c.nu(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides];(0,d.iQ)(this.poolSize,"poolSize"),(0,d.iQ)(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,T.cj)(this.dataFormat),(0,T.zb)(this.padding),this.inputSpec=[new o.Zg({ndim:4})]}computeOutputShape(t){t=(0,m.Wf)(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],s="channelsFirst"===this.dataFormat?t[3]:t[2];return e=M(e,this.poolSize[0],this.padding,this.strides[0]),s=M(s,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,s]:[t[0],e,s,t[3]]}call(t,e){return(0,i.lub)((()=>(this.invokeCallHook(t,e),this.poolingFunction((0,m.nQ)(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Pt extends Zt{constructor(t){super(t)}poolingFunction(t,e,s,i,n){return(0,T.cj)(n),(0,T.zb)(i),Bt(t,e,s,i,n,"max")}}Pt.className="MaxPooling2D",i.m7h.registerClass(Pt);class qt extends Zt{constructor(t){super(t)}poolingFunction(t,e,s,i,n){return(0,T.cj)(n),(0,T.zb)(i),Bt(t,e,s,i,n,"avg")}}qt.className="AveragePooling2D",i.m7h.registerClass(qt);class Gt extends o.mh{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new c.nu(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];(0,d.iQ)(this.poolSize,"poolSize"),(0,d.iQ)(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,T.cj)(this.dataFormat),(0,T.zb)(this.padding),this.inputSpec=[new o.Zg({ndim:5})]}computeOutputShape(t){t=(0,m.Wf)(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],s="channelsFirst"===this.dataFormat?t[3]:t[2],i="channelsFirst"===this.dataFormat?t[4]:t[3];return e=M(e,this.poolSize[0],this.padding,this.strides[0]),s=M(s,this.poolSize[1],this.padding,this.strides[1]),i=M(i,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,s,i]:[t[0],e,s,i,t[4]]}call(t,e){return(0,i.lub)((()=>(this.invokeCallHook(t,e),this.poolingFunction((0,m.nQ)(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Kt extends Gt{constructor(t){super(t)}poolingFunction(t,e,s,i,n){return(0,T.cj)(n),(0,T.zb)(i),Ht(t,e,s,i,n,"max")}}Kt.className="MaxPooling3D",i.m7h.registerClass(Kt);class Xt extends Gt{constructor(t){super(t)}poolingFunction(t,e,s,i,n){return(0,T.cj)(n),(0,T.zb)(i),Ht(t,e,s,i,n,"avg")}}Xt.className="AveragePooling3D",i.m7h.registerClass(Xt);class Yt extends o.mh{constructor(t){super(t),this.inputSpec=[new o.Zg({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new c.nj}}class te extends Yt{constructor(t){super(t||{})}call(t,e){return(0,i.lub)((()=>{const e=(0,m.nQ)(t);return i.J69(e,1)}))}}te.className="GlobalAveragePooling1D",i.m7h.registerClass(te);class ee extends Yt{constructor(t){super(t||{})}call(t,e){return(0,i.lub)((()=>{const e=(0,m.nQ)(t);return i.Fp7(e,1)}))}}ee.className="GlobalMaxPooling1D",i.m7h.registerClass(ee);class se extends o.mh{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,T.cj)(this.dataFormat),this.inputSpec=[new o.Zg({ndim:4})]}computeOutputShape(t){return"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new c.nj}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class ie extends se{call(t,e){return(0,i.lub)((()=>{const e=(0,m.nQ)(t);return"channelsLast"===this.dataFormat?i.J69(e,[1,2]):i.J69(e,[2,3])}))}}ie.className="GlobalAveragePooling2D",i.m7h.registerClass(ie);class ne extends se{call(t,e){return(0,i.lub)((()=>{const e=(0,m.nQ)(t);return"channelsLast"===this.dataFormat?i.Fp7(e,[1,2]):i.Fp7(e,[2,3])}))}}ne.className="GlobalMaxPooling2D",i.m7h.registerClass(ne);var re=s(44685);class ae extends o.mh{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e,s={}){const i=e.layer,n=(0,p.v)(i,s);delete e.layer;const r={layer:n};return Object.assign(r,e),new t(r)}}class le extends ae{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=(0,m.Wf)(t)).length<3)throw new c.nu(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(t)}`);this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=(0,m.Wf)(t))[0]].concat(t.slice(2)),s=this.layer.computeOutputShape(e),i=t[1];return[s[0],i].concat(s.slice(1))}call(t,e){return(0,i.lub)((()=>et(((t,s)=>[(0,m.nQ)(this.layer.call(t,e)),[]]),t=(0,m.nQ)(t),[],!1,null,null,!1,!0)[1]))}}le.className="TimeDistributed",i.m7h.registerClass(le);class oe extends ae{constructor(t){super(t);const e=t.layer.getConfig(),s={};s.className=t.layer.getClassName(),s.config=e,this.forwardLayer=(0,p.v)(s),e.goBackwards=!0!==e.goBackwards;const i={};var n;if(i.className=t.layer.getClassName(),i.config=e,this.backwardLayer=(0,p.v)(i),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,n=this.mergeMode,d.xn(re.eY,"BidirectionalMergeMode",n),t.weights)throw new c.nj("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,s=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,s)),this.backwardLayer.setWeights(t.slice(s))}computeOutputShape(t){let e,s,i,n=this.forwardLayer.computeOutputShape(t);return Array.isArray(n)&&Array.isArray(n[0])||(n=[n]),this.returnState?(i=n.slice(1),e=n[0]):e=n[0],"concat"===this.mergeMode?(e[e.length-1]*=2,s=[e]):s=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?s.concat(i).concat(i.slice()):[e].concat(i).concat(i.slice()):d.Bq(s)}apply(t,e){let s=null==e?null:e.initialState,i=null==e?null:e.constants;null==e&&(e={});const n=tt(t,s,i,this.numConstants);if(t=n.inputs,s=n.initialState,i=n.constants,Array.isArray(t)&&(s=t.slice(1),t=t[0]),(null==s||0===s.length)&&null==i)return super.apply(t,e);const r=[],a=[];if(null!=s){const t=s.length;if(t%2>0)throw new c.nu("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=s,r.push(...s);const i=s.map((t=>new o.Zg({shape:t.shape})));this.forwardLayer.stateSpec=i.slice(0,t/2),this.backwardLayer.stateSpec=i.slice(t/2),a.push(...i)}if(null!=i)throw new c.nj("Support for constants in Bidirectional layers is not implemented yet.");const l=r[0]instanceof o.Iy;for(const t of r)if(t instanceof o.Iy!==l)throw new c.nu("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(l){const s=[t].concat(r),i=this.inputSpec.concat(a),n=this.inputSpec;this.inputSpec=i;const l=super.apply(s,e);return this.inputSpec=n,l}return super.apply(t,e)}call(t,e){return(0,i.lub)((()=>{const s=e.initialState;let n,r,a,l;if(null==s)n=this.forwardLayer.call(t,e),r=this.backwardLayer.call(t,e);else{const i=s.slice(0,s.length/2),a=s.slice(s.length/2);n=this.forwardLayer.call(t,Object.assign(e,{initialState:i})),r=this.backwardLayer.call(t,Object.assign(e,{initialState:a}))}return this.returnState&&(Array.isArray(n)&&(a=n.slice(1).concat(r.slice(1))),n=n[0],r=r[0]),this.returnSequences&&(r=i.GYS(r,1)),"concat"===this.mergeMode?l=w.mV([n,r]):"sum"===this.mergeMode?l=i.IHx(n,r):"ave"===this.mergeMode?l=i.dC7(.5,i.IHx(n,r)):"mul"===this.mergeMode?l=i.dC7(n,r):null==this.mergeMode&&(l=[n,r]),this.returnState?null==this.mergeMode?l.concat(a):[l].concat(a):l}))}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){(0,T.f4)(this.forwardLayer.name,(()=>{this.forwardLayer.build(t)})),(0,T.f4)(this.backwardLayer.name,(()=>{this.backwardLayer.build(t)})),this.built=!0}computeMask(t,e){let s;if(Array.isArray(e)&&(e=e[0]),s=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map((t=>null));return Array.isArray(s)?s.concat(t).concat(t):[s].concat(t).concat(t)}return s}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const s=(0,p.v)(e.layer);if(delete e.layer,null!=e.numConstants)throw new c.nj("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const i=e;return i.layer=s,new t(i)}}oe.className="Bidirectional",i.m7h.registerClass(oe);class ue extends o.mh{constructor(t){super(t),this.scale=t.scale,t.offset?this.offset=t.offset:this.offset=0}getConfig(){const t={scale:this.scale,offset:this.offset},e=super.getConfig();return Object.assign(t,e),t}call(t,e){return(0,i.lub)((()=>("float32"!==(t=(0,m.nQ)(t)).dtype&&(t=w.pj(t,"float32")),(0,i.IHx)((0,i.dC7)(t,this.scale),this.offset))))}}ue.className="Rescaling",i.m7h.registerClass(ue);const{resizeBilinear:he,cropAndResize:ce}=i.BHj;class pe extends o.mh{constructor(t){super(t),this.height=t.height,this.width=t.width}centerCrop(t,e,s,n,r,a,l,o){return(0,i.lub)((()=>{let u,h=!1;const c=[e/a,s/l,(n+e)/a,(r+s)/l],p=[];3===t.rank?(h=!0,u=(0,i.knu)([t])):u=t;for(let t=0;t<u.shape[0];t++)p.push(c);const d=(0,i.XeE)(p,[p.length,4]),g=(0,i.w6H)(0,p.length,1,"int32"),f=ce(u,d,g,[n,r],"nearest");return h?w.pj((0,m.nQ)((0,i.HHK)(f)),o):w.pj(f,o)}))}upsize(t,e,s,n){return(0,i.lub)((()=>{const i=he(t,[e,s]);return w.pj(i,n)}))}call(t,e){return(0,i.lub)((()=>{const e=(0,m.nQ)(t),s=e.dtype,i=e.shape,n=i[i.length-3],r=i[i.length-2];let a=0;n!==this.height&&(a=Math.floor((n-this.height)/2));let l=0;return r!==this.width&&(l=Math.floor((r-this.width)/2),0===l&&(l=1)),a>=0&&l>=0?this.centerCrop(e,a,l,this.height,this.width,n,r,s):this.upsize(t,this.height,this.width,s)}))}getConfig(){const t={height:this.height,width:this.width},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){const e=(t=(0,m.Wf)(t)).length-3,s=t.length-2;return t[e]=this.height,t[s]=this.width,t}}pe.className="CenterCrop",i.m7h.registerClass(pe);class de extends o.mh{constructor(t){super(t),this.numTokens=t.numTokens,t.outputMode?this.outputMode=t.outputMode:this.outputMode="multiHot"}getConfig(){const t={numTokens:this.numTokens,outputMode:this.outputMode},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){return null==(t=(0,m.Wf)(t))?[this.numTokens]:"oneHot"===this.outputMode&&1!==t[t.length-1]?(t.push(this.numTokens),t):(t[t.length-1]=this.numTokens,t)}call(t,e){return(0,i.lub)((()=>{let s;if("int32"!==(t=(0,m.nQ)(t)).dtype&&(t=w.pj(t,"int32")),void 0!==e.countWeights){if("count"!==this.outputMode)throw new c.nu(`countWeights is not used when outputMode !== count.\n              Received countWeights=${e.countWeights}`);s=(0,m.nQ)(e.countWeights)}const n=(0,i.Fp7)(t),r=(0,i.VV$)(t),a=(0,i.pjt)(this.numTokens,n).bufferSync().get(0),l=(0,i.brS)(r,0).bufferSync().get(0);if(!a||!l)throw new c.nu(`Input values must be between 0 < values <= numTokens with numTokens=${this.numTokens}`);return function(t,e,s,n){let r=(0,m.nQ)(t);if("int32"!==r.dtype&&(r=w.pj(r,"int32")),"int"===e)return r;const a=r.shape;if(0===r.rank&&(r=(0,i.dt4)(r,-1)),"oneHot"===e&&1!==r.shape[r.shape.length-1]&&(r=(0,i.dt4)(r,-1)),r.rank>2)throw new c.nu(`When outputMode is not int, maximum output rank is 2 Received outputMode ${e} and input shape ${a} which would result in output rank ${r.rank}.`);const l=["multiHot","oneHot"].includes(e),o=r;let u;if(u=void 0!==n&&"count"===e?(0,i.ppE)(o,n,s,l):(0,i.ppE)(o,[],s,l),"tfIdf"!==e)return u;if(n)return(0,i.dC7)(u,n);throw new c.nu("When outputMode is 'tfIdf', weights must be provided.")}(t,this.outputMode,this.numTokens,s)}))}}de.className="CategoryEncoding",i.m7h.registerClass(de);const ge=new Set(["bilinear","nearest"]);class me extends o.mh{constructor(t){if(super(t),this.height=t.height,this.width=t.width,t.interpolation){if(!ge.has(t.interpolation))throw new c.nu(`Invalid interpolation parameter: ${t.interpolation} is not implemented`);this.interpolation=t.interpolation}else this.interpolation="bilinear";this.cropToAspectRatio=Boolean(t.cropToAspectRatio)}computeOutputShape(t){const e=(t=(0,m.Wf)(t))[2];return[this.height,this.width,e]}getConfig(){const t={height:this.height,width:this.width,interpolation:this.interpolation,cropToAspectRatio:this.cropToAspectRatio},e=super.getConfig();return Object.assign(t,e),t}call(t,e){return(0,i.lub)((()=>{const e=[this.height,this.width];if("bilinear"===this.interpolation)return i.BHj.resizeBilinear(t,e,!this.cropToAspectRatio);if("nearest"===this.interpolation)return i.BHj.resizeNearestNeighbor(t,e,!this.cropToAspectRatio);throw new Error(`Interpolation is ${this.interpolation} but only ${[...ge]} are supported`)}))}}me.className="Resizing",i.m7h.registerClass(me),s(38678),s(66084)},85654:(t,e,s)=>{s.d(e,{m7:()=>c,M6:()=>d,L5:()=>L,Cx:()=>F});var i=s(88478),n=s(39840),r=s(48090),a=s(40588);const l=["fanIn","fanOut","fanAvg"],o=["normal","uniform","truncatedNormal"];var u=s(2931),h=s(96040);class c extends i.m7h.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class p extends c{apply(t,e){return(0,i.lls)(t,e)}}p.className="Zeros",i.m7h.registerClass(p);class d extends c{apply(t,e){return(0,i.iUs)(t,e)}}d.className="Ones",i.m7h.registerClass(d);class g extends c{constructor(t){if(super(),"object"!=typeof t)throw new a.nu(`Expected argument of type ConstantConfig but got ${t}`);if(void 0===t.value)throw new a.nu(`config must have value set but got ${t}`);this.value=t.value}apply(t,e){return(0,i.lub)((()=>(0,i.dC7)((0,i.iD$)(this.value),(0,i.iUs)(t,e))))}getConfig(){return{value:this.value}}}g.className="Constant",i.m7h.registerClass(g);class m extends c{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,e){return(0,i.LGj)(t,this.minval,this.maxval,e,this.seed)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}m.className="RandomUniform",i.m7h.registerClass(m);class f extends c{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new a.nj(`randomNormal does not support dType ${e}.`);return n.nG(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}f.className="RandomNormal",i.m7h.registerClass(f);class b extends c{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new a.nj(`truncatedNormal does not support dType ${e}.`);return(0,i.Xu6)(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}b.className="TruncatedNormal",i.m7h.registerClass(b);class y extends c{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,e){return(0,i.lub)((()=>{if(2!==t.length||t[0]!==t[1])throw new a.nu("Identity matrix initializer can only be used for 2D square matrices.");return(0,i.dC7)(this.gain,(0,i.iyy)(t[0]))}))}getConfig(){return{gain:this.gain}}}y.className="Identity",i.m7h.registerClass(y);class w extends c{constructor(t){if(super(),t.scale<0)throw new a.nu(`scale must be a positive float. Got: ${t.scale}`);var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,(0,u.xn)(l,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){(0,u.xn)(o,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,e){const s=function(t,e="channelsLast"){let s,i;if((0,r.cj)(e),2===t.length)s=t[0],i=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===e){const e=(0,h.NS)(t,2);s=t[1]*e,i=t[0]*e}else if("channelsLast"===e){const e=(0,h.NS)(t,0,t.length-2);s=t[t.length-2]*e,i=t[t.length-1]*e}}else{const e=(0,h.NS)(t);s=Math.sqrt(e),i=Math.sqrt(e)}return[s,i]}(t),n=s[0],l=s[1];let o=this.scale;if("fanIn"===this.mode?o/=Math.max(1,n):"fanOut"===this.mode?o/=Math.max(1,l):o/=Math.max(1,(n+l)/2),"normal"===this.distribution){const s=Math.sqrt(o);if("float32"!==(e=e||"float32")&&"int32"!==e)throw new a.nj(`${this.getClassName()} does not support dType ${e}.`);return(0,i.Xu6)(t,0,s,e,this.seed)}{const s=Math.sqrt(3*o);return(0,i.LGj)(t,-s,s,e,this.seed)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}w.className="VarianceScaling",i.m7h.registerClass(w);class C extends w{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return w.className}}C.className="GlorotUniform",i.m7h.registerClass(C);class S extends w{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return w.className}}S.className="GlorotNormal",i.m7h.registerClass(S);class z extends w{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return w.className}}z.className="HeNormal",i.m7h.registerClass(z);class k extends w{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return w.className}}k.className="HeUniform",i.m7h.registerClass(k);class A extends w{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return w.className}}A.className="LeCunNormal",i.m7h.registerClass(A);class I extends w{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return w.className}}I.className="LeCunUniform",i.m7h.registerClass(I);class v extends c{constructor(t){if(super(),this.DEFAULT_GAIN=1,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed,null!=this.seed)throw new a.nj("Random seed is not implemented for Orthogonal Initializer yet.")}apply(t,e){return(0,i.lub)((()=>{if(t.length<2)throw new a.nj("Shape must be at least 2D.");t[0]*t[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${t[0]*t[1]}) elements: Slowness may result.`);const e=t[0]>t[1]?[t[1],t[0]]:t,s=n.nG(e,0,1,"float32");let r=i.$r2.gramSchmidt(s);return t[0]>t[1]&&(r=(0,i.p4s)(r)),(0,i.dC7)(this.gain,r)}))}getConfig(){return{gain:this.gain,seed:this.seed}}}v.className="Orthogonal",i.m7h.registerClass(v);const N={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function x(t,e={}){return(0,u.tU)(t,i.m7h.SerializationMap.getMap().classNameMap,e,"initializer")}function F(t){return(0,u.Kj)(t)}function L(t){if("string"==typeof t){const e=t in N?N[t]:t;if("GlorotNormal"===e)return new S;if("GlorotUniform"===e)return new C;if("HeNormal"===e)return new z;if("HeUniform"===e)return new k;if("LeCunNormal"===e)return new A;if("LeCunUniform"===e)return new I;{const t={};return t.className=e,t.config={},x(t)}}return t instanceof c?t:x(t)}},44685:(t,e,s)=>{s.d(e,{MK:()=>a,Mz:()=>n,PS:()=>i,eY:()=>l,zx:()=>r});const i=["channelsFirst","channelsLast"],n=["nearest","bilinear"],r=["valid","same","causal"],a=["max","avg"],l=["sum","mul","concat","ave"]},49897:(t,e,s)=>{s.d(e,{v:()=>r});var i=s(88478),n=s(2931);function r(t,e={},s=!1){return(0,n.tU)(t,i.m7h.SerializationMap.getMap().classNameMap,e,"layer",s)}},73146:(t,e,s)=>{s.d(e,{Z:()=>n,i:()=>r});var i=s(88478);async function n(t){if(null==t)return;const e=[],s=[],n=[];for(const i in t){const r=t[i];if("number"!=typeof r){const t=r;e.push(t.data()),s.push(i),n.push(t)}}if(e.length>0){const r=await Promise.all(e);for(let e=0;e<r.length;++e)t[s[e]]=r[e][0];(0,i.B90)(n)}}function r(t){if(null!=t)for(const e in t){const s=t[e];"number"!=typeof s&&s.dispose()}}},86275:(t,e,s)=>{s.d(e,{Eq:()=>l,FD:()=>o,KM:()=>p,Ls:()=>g,U2:()=>f,dr:()=>m,fO:()=>d,ke:()=>u,t3:()=>h,uq:()=>c});var i=s(88478),n=s(12012),r=s(39840),a=s(40588);function l(t,e){return(0,i.lub)((()=>{"float32"!==t.dtype&&(t=i.pju(t,"float32"));const s=i.Smz(r.h6(t),e,!0),a=i.hlL(s.shape,(0,n.Ho)()),l=i._b3(i.gWQ(s,a));return i.hiC(t,l)}))}function o(t,e){return(0,i.lub)((()=>i.J69(r.h6(i.luU(e,t)),-1)))}function u(t,e){return(0,i.lub)((()=>i.J69(i.WnP(i.luU(e,t)),-1)))}function h(t,e){return(0,i.lub)((()=>{const s=i.luU(t,e),r=i.iUl(i.WnP(t),(0,n.Ho)(),Number.MAX_VALUE),a=i.WnP(i.hiC(s,r));return i.dC7(100,i.J69(a,-1))}))}function c(t,e,s=!1){return(0,i.lub)((()=>{if(s)e=i.XAC(e);else{const t=i.Smz(e,e.shape.length-1,!0);e=i.hiC(e,t)}return e=i.iUl(e,(0,n.Ho)(),1-(0,n.Ho)()),i.W76(i.Smz(i.dC7(i.pju(t,"float32"),i.cM7(e)),e.shape.length-1))}))}function p(t,e,s=!1){return(0,i.lub)((()=>{const a=i.pju(i.GWj(r.xH(t)),"int32"),l=(e=i.iUl(e,(0,n.Ho)(),1-(0,n.Ho)())).shape;return c(i.XLQ(i.lfX(a,l[l.length-1]),l),e,s)}))}function d(t,e){return(0,i.lub)((()=>{let s;return s=i.iUl(e,(0,n.Ho)(),1-(0,n.Ho)()),s=i.cM7(i.hiC(s,i.luU(1,s))),i.J69(function(t,e){if(!i.D5U.arraysEqual(t.shape,e.shape))throw new a.nu(`logits and labels must have the same shape, but got shapes ${JSON.stringify(t.shape)} and ${JSON.stringify(e.shape)}`);return(0,i.lub)((()=>{const s=i.UYe(e),n=i.W76(i.WnP(e));return i.IHx(i.luU(s,i.dC7(e,t)),i.Krr(i.Qqt(n)))}))}(t,s),-1)}))}function g(t,e){return(0,i.lub)((()=>{const s=l(t,-1),n=l(e,-1),r=i.dC7(s,n);return i.W76(i.Smz(r,-1))}))}const m={meanSquaredError:o,meanAbsoluteError:u,meanAbsolutePercentageError:h,meanSquaredLogarithmicError:function(t,e){return(0,i.lub)((()=>{const s=i.iUl(e,(0,n.Ho)(),Number.MAX_VALUE),a=i.cM7(i.IHx(1,s)),l=i.iUl(t,(0,n.Ho)(),Number.MAX_VALUE),o=i.cM7(i.IHx(1,l));return i.J69(r.h6(i.luU(a,o)),-1)}))},squaredHinge:function(t,e){return(0,i.lub)((()=>{const s=i.gWQ(0,i.luU(1,i.dC7(t,e)));return i.J69(r.h6(s),-1)}))},hinge:function(t,e){return(0,i.lub)((()=>{const s=i.gWQ(0,i.luU(1,i.dC7(t,e)));return i.J69(s,-1)}))},categoricalHinge:function(t,e){return(0,i.lub)((()=>{const s=i.Smz(i.dC7(t,e),-1),n=i.Fp7(i.dC7(i.luU(1,t),e),-1);return i.gWQ(0,i.IHx(1,i.luU(n,s)))}))},logcosh:function(t,e){return(0,i.lub)((()=>{const s=Math.log(2),n=i.luU(e,t),r=i.luU(i.IHx(n,i.Wvh(i.dC7(-2,n))),s);return i.J69(r,-1)}))},categoricalCrossentropy:c,sparseCategoricalCrossentropy:p,binaryCrossentropy:d,kullbackLeiblerDivergence:function(t,e){return(0,i.lub)((()=>{const s=i.iUl(t,(0,n.Ho)(),1),r=i.iUl(e,(0,n.Ho)(),1);return i.Smz(i.dC7(t,i.cM7(i.hiC(s,r))),-1)}))},poisson:function(t,e){return(0,i.lub)((()=>{const s=i.cM7(i.IHx((0,n.Ho)(),e));return i.J69(i.luU(e,i.dC7(t,s)),-1)}))},cosineProximity:g};function f(t){if("string"==typeof t){if(t in m)return m[t];let e=`Unknown loss ${t}`;throw t.toLowerCase().includes("softmaxcrossentropy")&&(e=`Unknown loss ${t}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new a.nu(e)}return t}},38678:(t,e,s)=>{s.d(e,{G5:()=>u,KM:()=>C,TY:()=>c,U2:()=>z,_F:()=>o,aI:()=>k,fO:()=>h,uq:()=>y});var i=s(88478),n=s(39840),r=s(40588),a=s(86275),l=s(2931);function o(t,e){return(0,i.lub)((()=>{const s=i.dC7(.5,i.JpU(e)),r=n.pj(i.pjt(e,s),t.dtype);return i.J69(i.DgJ(t,r),-1)}))}function u(t,e){return(0,i.lub)((()=>n.pj(i.DgJ(i.NqF(t,-1),i.NqF(e,-1)),"float32")))}function h(t,e){return(0,a.fO)(t,e)}function c(t,e){return t.rank===e.rank&&(t=i.L9e(t,[t.rank-1])),(e=i.NqF(e,-1)).dtype!==t.dtype&&(e=i.pju(e,t.dtype)),i.pju(i.DgJ(t,e),"float32")}const p=a.FD,d=a.FD,g=a.ke,m=a.ke,f=a.t3,b=a.t3,y=a.uq,w=a.Ls,C=a.KM,S={binaryAccuracy:o,categoricalAccuracy:u,precision:function(t,e){return(0,i.lub)((()=>{const s=function(t,e){return(0,i.lub)((()=>i.pju(i.Smz(i.HvI(i.DgJ(t,1),i.DgJ(e,1))),"float32")))}(t,e),n=function(t,e){return(0,i.lub)((()=>i.pju(i.Smz(i.HvI(i.DgJ(t,0),i.DgJ(e,1))),"float32")))}(t,e),r=i.IHx(s,n);return i.pju(i.arb(i.pjt(r,0),i.hiC(s,r),0),"float32")}))},categoricalCrossentropy:y,sparseCategoricalCrossentropy:C,mse:p,MSE:d,mae:g,MAE:m,mape:f,MAPE:b,cosine:w};function z(t){if("string"==typeof t&&t in S)return S[t];if("string"!=typeof t&&null!=t)return t;throw new r.nu(`Unknown metric ${t}`)}function k(t){if(l.hu(null!==t,`Unknown LossOrMetricFn ${t}`),"string"==typeof t)return t;{let e;for(const s of Object.keys(a.dr))if(a.dr[s]===t){e=s;break}if(void 0!==e)return e;for(const s of Object.keys(S))if(S[s]===t){e=s;break}return void 0!==e?e:t.name}}},92328:(t,e,s)=>{s.d(e,{j:()=>a});var i=s(88478),n=s(12012),r=s(40588);function a(t){const e={Adagrad:()=>i.p_j.adagrad(.01),Adadelta:()=>i.p_j.adadelta(1,.95,(0,n.Ho)()),Adam:()=>i.p_j.adam(.001,.9,.999,(0,n.Ho)()),Adamax:()=>i.p_j.adamax(.002,.9,.999,(0,n.Ho)(),0),RMSProp:()=>i.p_j.rmsprop(.001,.9,0,(0,n.Ho)()),SGD:()=>i.p_j.sgd(.01)};if(e.adagrad=e.Adagrad,e.adadelta=e.Adadelta,e.adam=e.Adam,e.adamax=e.Adamax,e.rmsprop=e.RMSProp,e.sgd=e.SGD,t in e)return e[t]();throw new r.nu(`Unknown Optimizer ${t}`)}},38374:(t,e,s)=>{s.d(e,{WE:()=>i});function i(t,e,s=!1){if(null==t||"object"!=typeof t||Object.getPrototypeOf(t)!==Object.prototype||!n(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(s){const s=JSON.stringify(t);s.length>1048576&&console.warn(`User-defined metadata of model "${e}" is too large in size (length=${s.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= 1048576.`)}}function n(t){if(null===t)return!0;if("object"==typeof t){if(Object.getPrototypeOf(t)===Object.prototype){const e=Object.keys(t);for(const s of e){if("string"!=typeof s)return!1;if(!n(t[s]))return!1}return!0}if(Array.isArray(t)){for(const e of t)if(!n(e))return!1;return!0}return!1}{const e=typeof t;return"string"===e||"number"===e||"boolean"===e}}},6247:(t,e,s)=>{s.d(e,{s:()=>i});class i{constructor(t){this.maxEntries=t||100,this.cache=new Map}get(t){let e;return this.cache.has(t)&&(e=this.cache.get(t),this.cache.delete(t),this.cache.set(t,e)),e}put(t,e){if(this.cache.has(t))this.cache.delete(t);else if(this.cache.size>=this.maxEntries){const t=this.cache.keys().next().value;this.cache.delete(t)}this.cache.set(t,e)}getMaxEntries(){return this.maxEntries}setMaxEntries(t){if(t<0)throw new Error(`The maxEntries of LRU caches must be at least 0, but got ${t}.`);if(this.maxEntries>t)for(let e=0;e<this.maxEntries-t;e++){const t=this.cache.keys().next().value;this.cache.delete(t)}this.maxEntries=t}}},2931:(t,e,s)=>{s.d(e,{Bq:()=>o,D1:()=>h,Ds:()=>k,JE:()=>r,Kj:()=>d,L7:()=>f,Mx:()=>C,QX:()=>l,Tw:()=>b,WT:()=>A,hu:()=>a,iQ:()=>S,nK:()=>y,tU:()=>m,xn:()=>w,zW:()=>c,zZ:()=>u});var i=s(88478),n=s(40588);function r(t,e){if(Array.isArray(t)){let s=[];for(let i=0;i<e;i++)s=s.concat(t);return s}{const s=new Array(e);return s.fill(t),s}}function a(t,e){if(!t)throw new n.ps(e)}function l(t,e){let s=0;for(const i of t)i===e&&s++;return s}function o(t){return 1===t.length?t[0]:t}function u(t){return Array.isArray(t)?t:[t]}function h(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function c(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,((t,e)=>e.toUpperCase()))}let p={};function d(t){if(null==t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function g(t){if(null!=t&&"object"==typeof t)if(Array.isArray(t))t.forEach((t=>g(t)));else{const e=Object.keys(t);for(const s of e){const e=t[s];null!=e&&"object"==typeof e&&(Array.isArray(e)||"ndarray"!==e.type||"number"!=typeof e.value?g(e):t[s]=e.value)}}}function m(t,e={},s={},i="object",r=!1){if("string"==typeof t){const r=t;let a;if(r in s)a=s[r];else if(r in p)a=p[r];else if(a=e[r],null==a)throw new n.nu(`Unknown ${i}: ${t}. This may be due to one of the following reasons:\n1. The ${i} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${i} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return a}{const a=t;if(null==a.className||null==a.config)throw new n.nu(`${i}: Improper config format: ${JSON.stringify(a)}.\n'className' and 'config' must set.`);const l=a.className;let o,u;if(l in s?[o,u]=s[l]:l in p?[o,u]=p.className:l in e&&([o,u]=e[l]),null==o)throw new n.nu(`Unknown ${i}: ${l}. This may be due to one of the following reasons:\n1. The ${i} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${i} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=u){const t={};for(const e of Object.keys(p))t[e]=p[e];for(const e of Object.keys(s))t[e]=s[e];a.config.customObjects=t;const e=Object.assign({},p);for(const t of Object.keys(s))p[t]=s[t];g(a.config);const i=u(o,a.config,s,r);return p=Object.assign({},e),i}{const t=Object.assign({},p);for(const t of Object.keys(s))p[t]=s[t];const e=new o(a.config);return p=Object.assign({},t),e}}}function f(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function b(t){if(null==t)return t;const e=[];for(const s of t)-1===e.indexOf(s)&&e.push(s);return e}function y(t){if(null==t)throw new n.nu(`Invalid value in obj: ${JSON.stringify(t)}`);for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function w(t,e,s){if(null!=s&&t.indexOf(s)<0)throw new n.nu(`${s} is not a valid ${e}.  Valid values are ${t} or null/undefined.`)}function C(t,e,s=0,i=1/0){return a(s>=0),a(i>=s),Array.isArray(t)&&t.length>=s&&t.length<=i&&t.every((t=>typeof t===e))}function S(t,e){Array.isArray(t)?(i.D5U.assert(t.length>0,(()=>`${e} is unexpectedly an empty array.`)),t.forEach(((t,s)=>S(t,`element ${s+1} of ${e}`)))):i.D5U.assert(Number.isInteger(t)&&t>0,(()=>`Expected ${e} to be a positive integer, but got ${z(t)}.`))}function z(t){return null===t?"null":Array.isArray(t)?"["+t.map((t=>z(t))).join(",")+"]":"string"==typeof t?`"${t}"`:`${t}`}function k(t,e,s){let n,r=null!=s?s():i.D5U.now();return(...a)=>{const l=null!=s?s():i.D5U.now();return l-r<e||(r=l,n=t(...a)),n}}function A(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}},30618:(t,e,s)=>{s.d(e,{I:()=>n});var i=s(23013);function n(t,e,s,n=console.log){const o=function(t){let e=!0;const s=[],i=[];for(const e in t.nodesByDepth)s.push(t.nodesByDepth[e]);for(const t of s){if(t.length>1||1===t.length&&t[0].inboundLayers.length>1){e=!1;break}i.push(...t)}if(e)for(const s of t.layers){let t=!1;for(const n of s.inboundNodes)if(-1!==i.indexOf(n)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),u=["Layer (type)","Input Shape","Output shape","Param #"];let h;if(o?(e=e||90,s=s||[.32,.61,.89,1]):(e=e||115,s=s||[.24,.48,.7,.8,1]),s[s.length-1]<=1&&(s=s.map((t=>Math.floor(e*t)))),!o){u.push("Receives inputs"),h=[];for(const e in t.nodesByDepth)h.push(...t.nodesByDepth[e])}n("_".repeat(e)),r(u,s,n),n("=".repeat(e));const c=t.layers;for(let t=0;t<c.length;++t)o?a(c[t],s,n):l(c[t],s,h,n),n((t===c.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const p=function(t){let e;return e=null!=t.collectedTrainableWeights?(0,i.t)(t.collectedTrainableWeights):(0,i.t)(t.trainableWeights),e}(t),d=(0,i.t)(t.nonTrainableWeights);n(`Total params: ${p+d}`),n(`Trainable params: ${p}`),n(`Non-trainable params: ${d}`),n("_".repeat(e))}function r(t,e,s=console.log){let i="";for(let s=0;s<t.length;++s)s>0&&(i=i.slice(0,i.length-1)+" "),i+=t[s],i=i.slice(0,e[s]),i+=" ".repeat(e[s]-i.length);s(i)}function a(t,e,s){let i,n;try{n=t.inboundNodes.map((t=>JSON.stringify(t.inputShapes))).join(",")}catch(t){n="multiple"}try{i=JSON.stringify(t.outputShape)}catch(t){i="multiple"}r([`${t.name} (${t.getClassName()})`,n,i,t.countParams().toString()],e,s)}function l(t,e,s,i){let n,a;try{a=t.inboundNodes.map((t=>JSON.stringify(t.inputShapes))).join(",")}catch(t){a="multiple"}try{n=JSON.stringify(t.outputShape)}catch(t){n="multiple"}const l=[];for(const e of t.inboundNodes)if(!(null!=s&&s.length>0&&-1===s.indexOf(e)))for(let t=0;t<e.inboundLayers.length;++t){const s=e.inboundLayers[t].name,i=e.nodeIndices[t],n=e.tensorIndices[t];l.push(`${s}[${i}][${n}]`)}const o=t.name,u=t.getClassName(),h=0===l.length?"":l[0];r([`${o} (${u})`,a,n,t.countParams().toString(),h],e,i);for(let t=1;t<l.length;++t)r(["","","","",l[t]],e,i)}},96040:(t,e,s)=>{s.d(e,{Fp:()=>l,NS:()=>r,U:()=>n,VV:()=>a,w6:()=>o});var i=s(40588);function n(t){return t===parseInt(t.toString(),10)}function r(t,e,s){null==e&&(e=0),null==s&&(s=t.length);let i=1;for(let n=e;n<s;++n)i*=t[n];return i}function a(t){if(0===t.length)return Number.NaN;let e=Number.POSITIVE_INFINITY;for(let s=0;s<t.length;s++){const i=t[s];i<e&&(e=i)}return e}function l(t){if(0===t.length)return Number.NaN;let e=Number.NEGATIVE_INFINITY;for(let s=0;s<t.length;s++){const i=t[s];i>e&&(e=i)}return e}function o(t,e){if(e<t)throw new i.nu(`end (${e}) < begin (${t}) is forbidden.`);const s=[];for(let i=t;i<e;++i)s.push(i);return s}},51977:(t,e,s)=>{s.d(e,{a:()=>r,q:()=>a});var i=s(2931);function n(t,e,s){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"==typeof s}function r(t,e){if(null===t)return null;if("string"==typeof t)return i.zW(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const s=[],i=t.length;for(let a=0;a<i;++a){const i=t[a];n(e,a,i)?s.push(i):s.push(r(i,e))}return s}{const e={};for(const s of Object.keys(t)){const n=t[s];if("name"===s&&"string"==typeof n)e[s]=n;else{const t=i.zW(s);e[t]=r(n,t)}}return e}}function a(t,e){if(null==t)return null;if("string"==typeof t)return i.D1(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const s=[],i=t.length;for(let r=0;r<i;++r){const i=t[r];n(e,r,i)?s.push(i):s.push(a(i,e))}return s}{const e={};for(const s of Object.keys(t)){const n=t[s];e[i.D1(s)]="name"!==s&&"className"!==s||"string"!=typeof n?a(n,s):n}return e}}},87538:(t,e,s)=>{s.d(e,{Wf:()=>l,XO:()=>n,nQ:()=>a,x6:()=>r});var i=s(40588);function n(t){return Array.isArray(t)&&Array.isArray(t[0])}function r(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function a(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new i.nu(`Expected Tensor length to be 1; got ${t.length}`);e=t[0]}else e=t;return e}function l(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return t[0];throw new i.nu(`Expected exactly 1 Shape; got ${t.length}`)}return t}},23013:(t,e,s)=>{function i(t){let e=0;for(const s of t)0===s.shape.length?e+=1:e+=s.shape.reduce(((t,e)=>t*e));return e}s.d(e,{t:()=>i})},41653:(t,e,s)=>{s.d(e,{FQ:()=>o,fU:()=>l,zb:()=>u});var i=s(88478),n=s(79608),r=s(48090);s(40588);const a="Variable";class l{constructor(t,e="float32",s=a,l=!0,o=null){this.dtype=null==e?"float32":e,this.shape=t.shape,this.id=(0,n.L)(),s=null==s?a:s,this.originalName=(0,r.MU)(s),this.name=(0,r.w8)(this.originalName),this.trainable_=l,this.constraint=o,this.val=i.VD$(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function o(t){return t.map((t=>t.read()))}function u(t){t.forEach((t=>{t[0].write(t[1])}))}},77385:(t,e,s)=>{s.d(e,{i:()=>i});const i="4.2.0"}}]);